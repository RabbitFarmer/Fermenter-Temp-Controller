#!/usr/bin/env python3
"""
app.py - FermenterApp main Flask application.

This file provides the full Flask app used in the conversation:
- BLE scanning (BleakScanner) if available
- Per-brew JSONL files under batches/{brewid}.jsonl (migrates legacy batch_{COLOR}_{BREWID}_{MMDDYYYY}.jsonl)
- Restricted control log in temp_control_log.jsonl
- Kasa worker integration (if kasa_worker available)
- Per-batch append_sample_to_batch_jsonl and forward_to_third_party_if_configured
- Chart Plotly page and /chart_data/<identifier> endpoint
- UI routes: dashboard, tilt_config, batch_settings, temp_config, update_temp_config, temp_report,
  export_temp_csv, scan_kasa_plugs, live_snapshot, reset_logs, exit_system, system_config
- Program entry runs Flask on 0.0.0.0:5000 in debug mode (when run directly)
"""

import asyncio
import hashlib
import json
import os
import queue
import re
import shutil
import smtplib
import threading
import time
from collections import deque, defaultdict
from datetime import datetime
from glob import glob as glob_func
from math import ceil
from multiprocessing import Process, Queue
import multiprocessing  # Needed for set_start_method and get_all_start_methods
from urllib.parse import urlparse
import subprocess
import signal
import webbrowser

from email.mime.text import MIMEText
from flask import (Flask, abort, jsonify, redirect, render_template, request,
                   url_for, make_response)

# Optional imports
try:
    from bleak import BleakScanner
except Exception:
    BleakScanner = None

try:
    from tilt_static import TILT_UUIDS, COLOR_MAP
except Exception:
    TILT_UUIDS = {}
    COLOR_MAP = {}

try:
    from kasa_worker import kasa_worker, kasa_query_state
except Exception:
    kasa_worker = None
    kasa_query_state = None

try:
    import requests
except Exception:
    requests = None

# Optional psutil for process management
try:
    import psutil
except Exception:
    psutil = None

# Import log_error for kasa error logging
try:
    from logger import log_error
except Exception:
    def log_error(msg):
        # Fallback if logger is not available
        print(f"[ERROR] {msg}")

app = Flask(__name__)

# --- Files and global constants ---------------------------------------------
LOG_PATH = 'temp_control/temp_control_log.jsonl'        # control events only
BATCHES_DIR = 'batches'                    # per-batch jsonl files live here
PER_PAGE = 30

# Config files
TILT_CONFIG_FILE = 'config/tilt_config.json'
TEMP_CFG_FILE = 'config/temp_control_config.json'
SYSTEM_CFG_FILE = 'config/system_config.json'

# Valid tab names for system config page (using set for O(1) lookup)
VALID_SYSTEM_CONFIG_TABS = {'main-settings', 'push-email', 'logging-integrations', 'backup-restore'}

# Chart caps
DEFAULT_CHART_LIMIT = 3000
MAX_CHART_LIMIT = 3000
MAX_ALL_LIMIT = 10000
MAX_FILENAME_LENGTH = 50

# --- Initialize config files from templates if they don't exist -------------
def ensure_config_files():
    """
    Ensure config files exist by copying from templates if needed.
    This prevents rsync/git pull from overwriting user's configuration data.
    """
    config_files = [
        ('config/system_config.json', 'config/system_config.json.template'),
        ('config/temp_control_config.json', 'config/temp_control_config.json.template'),
        ('config/tilt_config.json', 'config/tilt_config.json.template')
    ]
    
    for config_file, template_file in config_files:
        if not os.path.exists(config_file):
            if os.path.exists(template_file):
                try:
                    shutil.copy2(template_file, config_file)
                    print(f"[INIT] Created {config_file} from {template_file}")
                except Exception as e:
                    print(f"[INIT] Error copying {template_file} to {config_file}: {e}")
            else:
                print(f"[INIT] Warning: Neither {config_file} nor {template_file} exists")

ensure_config_files()

# --- Stop other app.py processes on startup --------------------------------
def stop_other_app_py():
    current_pid = os.getpid()
    stopped = []
    errors = []
    if psutil:
        try:
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    pid = proc.info['pid']
                    if pid == current_pid:
                        continue
                    cmdline = proc.info.get('cmdline') or []
                    name = proc.info.get('name') or ''
                    if any('app.py' in str(p) for p in cmdline) or 'app.py' in name:
                        try:
                            proc.terminate()
                            proc.wait(timeout=5)
                            stopped.append(pid)
                        except Exception:
                            try:
                                proc.kill()
                                stopped.append(pid)
                            except Exception as e:
                                errors.append((pid, str(e)))
                except Exception:
                    continue
            return {"stopped": stopped, "errors": errors}
        except Exception as e:
            errors.append(("psutil_iter", str(e)))

    try:
        pgrep = subprocess.run(['pgrep', '-f', 'app.py'], capture_output=True, text=True)
        if pgrep.returncode == 0:
            for line in pgrep.stdout.splitlines():
                try:
                    pid = int(line.strip())
                except Exception:
                    continue
                if pid == current_pid:
                    continue
                try:
                    os.kill(pid, signal.SIGTERM)
                    stopped.append(pid)
                except Exception as e:
                    try:
                        os.kill(pid, signal.SIGKILL)
                        stopped.append(pid)
                    except Exception as e2:
                        errors.append((pid, f"{e} / {e2}"))
    except Exception as e:
        errors.append(("pgrep", str(e)))

    return {"stopped": stopped, "errors": errors}

try:
    stopped_info = stop_other_app_py()
    print(f"[STARTUP] stop_other_app_py result: {stopped_info}")
except Exception as e:
    print(f"[STARTUP] startup housekeeping failed: {e}")

# --- Utilities --------------------------------------------------------------
def load_json(path, fallback):
    try:
        with open(path, 'r') as f:
            return json.load(f)
    except Exception:
        return fallback

def save_json(path, data):
    """
    Save data to a JSON file with pretty formatting.
    
    Creates parent directories if they don't exist.
    
    Args:
        path (str): Path to the JSON file
        data: Data to serialize to JSON
        
    Returns:
        bool: True if save succeeded, False if there was an error
    """
    try:
        # Ensure the directory exists
        directory = os.path.dirname(path)
        if directory and not os.path.exists(directory):
            os.makedirs(directory, exist_ok=True)
        
        with open(path, 'w') as f:
            json.dump(data, f, indent=2)
        return True
    except Exception as e:
        print(f"[LOG] Error saving JSON to {path}: {e}")
        return False

# --- New: Append batch metadata to batch jsonl ------------------------------
def append_batch_metadata_to_batch_jsonl(color, batch_entry):
    """Append a batch_metadata event to the relevant batch JSONL file."""
    brewid = batch_entry.get("brewid")
    if not color or not brewid:
        return False
    path = batch_jsonl_filename(color, brewid)
    entry = {
        "event": "batch_metadata",
        "payload": dict(batch_entry, timestamp=datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
    }
    try:
        with open(path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry) + "\n")
        return True
    except Exception as e:
        print(f"[LOG] Could not append batch_metadata for {color}: {e}")
        return False

# --- Restricted control-log writer -----------------------------------------
ALLOWED_EVENTS = {
    "tilt_reading": "SAMPLE",
    "heating_on": "HEATING-PLUG TURNED ON",
    "heating_off": "HEATING-PLUG TURNED OFF",
    "cooling_on": "COOLING-PLUG TURNED ON",
    "cooling_off": "COOLING-PLUG TURNED OFF",
    "temp_below_low_limit": "TEMP BELOW LOW LIMIT",
    "temp_above_high_limit": "TEMP ABOVE HIGH LIMIT",
    "temp_in_range": "IN RANGE",
    "temp_control_mode": "MODE_SELECTED",
    "temp_control_mode_changed": "MODE_CHANGED",
    "temp_control_started": "TEMP CONTROL STARTED",
    "temp_control_safety_shutdown": "SAFETY SHUTDOWN - CONTROL TILT INACTIVE",
}

# Create a set of allowed event values for O(1) lookup performance
ALLOWED_EVENT_VALUES = set(ALLOWED_EVENTS.values())

def _format_control_log_entry(event_type, payload):
    ts = datetime.utcnow()
    iso_ts = ts.replace(microsecond=0).isoformat() + "Z"
    date = ts.strftime("%Y-%m-%d")
    time_str = ts.strftime("%H:%M:%S")

    tilt_color = ""
    try:
        if isinstance(payload, dict):
            tilt_color = payload.get("tilt_color") or payload.get("tilt") or payload.get("color") or ""
    except Exception:
        tilt_color = ""

    def _to_float(val):
        try:
            if val is None or val == "":
                return None
            return float(val)
        except Exception:
            return None

    low = _to_float(payload.get("low_limit") if isinstance(payload, dict) else None)
    high = _to_float(payload.get("high_limit") if isinstance(payload, dict) else None)

    cur = None
    grav = None
    if isinstance(payload, dict):
        cur = payload.get("current_temp")
        if cur is None:
            cur = payload.get("temp_f") if payload.get("temp_f") is not None else payload.get("temp")
        grav = payload.get("gravity") or payload.get("grav") or payload.get("sg")

    cur = _to_float(cur)
    grav = _to_float(grav)

    event_label = ALLOWED_EVENTS.get(event_type, event_type)
    
    # Get brewid from tilt config if we have a tilt_color
    brewid = None
    if tilt_color and 'tilt_cfg' in globals():
        try:
            brewid = tilt_cfg.get(tilt_color, {}).get('brewid')
        except Exception:
            brewid = None

    entry = {
        "timestamp": iso_ts,
        "date": date,
        "time": time_str,
        "tilt_color": tilt_color,
        "brewid": brewid,
        "low_limit": low,
        "current_temp": cur,
        "temp_f": cur,
        "gravity": grav,
        "high_limit": high,
        "event": event_label
    }
    return entry

def append_control_log(event_type, payload):
    if event_type not in ALLOWED_EVENTS:
        return
    enable_heat = bool(temp_cfg.get("enable_heating")) if 'temp_cfg' in globals() else False
    enable_cool = bool(temp_cfg.get("enable_cooling")) if 'temp_cfg' in globals() else False
    if not (enable_heat or enable_cool):
        return
    try:
        d = os.path.dirname(LOG_PATH)
        if d and not os.path.exists(d):
            os.makedirs(d, exist_ok=True)
        entry = _format_control_log_entry(event_type, payload or {})
        with open(LOG_PATH, 'a') as f:
            f.write(json.dumps(entry) + "\n")
    except Exception as e:
        print(f"[LOG] Failed to append to control log: {e}")

@app.template_filter('localtime')
def localtime_filter(iso_str):
    from datetime import datetime, timezone
    try:
        if not iso_str:
            return ''
        if isinstance(iso_str, datetime):
            dt = iso_str
        else:
            s = str(iso_str)
            if s.endswith('Z'):
                try:
                    dt = datetime.fromisoformat(s.rstrip('Z')).replace(tzinfo=timezone.utc)
                except Exception:
                    return s
            else:
                try:
                    dt = datetime.fromisoformat(s)
                except Exception:
                    try:
                        dt = datetime.strptime(s, "%Y-%m-%dT%H:%M:%S")
                        dt = dt.replace(tzinfo=timezone.utc)
                    except Exception:
                        return s

        if dt.tzinfo is None or dt.tzinfo.utcoffset(dt) is None:
            dt = dt.replace(tzinfo=timezone.utc)

        local_tz = datetime.now().astimezone().tzinfo
        local_dt = dt.astimezone(local_tz)
        return local_dt.strftime('%Y-%m-%d %I:%M:%S %p')
    except Exception:
        return iso_str

# --- Load configs ----------------------------------------------------------
tilt_cfg = load_json(TILT_CONFIG_FILE, {})
temp_cfg = load_json(TEMP_CFG_FILE, {})
system_cfg = load_json(SYSTEM_CFG_FILE, {})

def ensure_temp_defaults():
    temp_cfg.setdefault("current_temp", None)
    temp_cfg.setdefault("low_limit", 0.0)
    temp_cfg.setdefault("high_limit", 0.0)
    temp_cfg.setdefault("enable_heating", False)
    temp_cfg.setdefault("enable_cooling", False)
    temp_cfg.setdefault("heating_plug", "")
    temp_cfg.setdefault("cooling_plug", "")
    temp_cfg.setdefault("heater_on", False)
    temp_cfg.setdefault("cooler_on", False)
    temp_cfg.setdefault("heater_pending", False)
    temp_cfg.setdefault("cooler_pending", False)
    temp_cfg.setdefault("heating_error", False)
    temp_cfg.setdefault("cooling_error", False)
    temp_cfg.setdefault("heating_error_notified", False)
    temp_cfg.setdefault("cooling_error_notified", False)
    temp_cfg.setdefault("notifications_trigger", False)
    temp_cfg.setdefault("notification_last_sent", None)
    temp_cfg.setdefault("notification_comm_failure", False)
    temp_cfg.setdefault("push_error", False)
    temp_cfg.setdefault("email_error", False)
    temp_cfg.setdefault("control_initialized", False)
    temp_cfg.setdefault("last_logged_low_limit", temp_cfg.get("low_limit"))
    temp_cfg.setdefault("last_logged_high_limit", temp_cfg.get("high_limit"))
    temp_cfg.setdefault("last_logged_enable_heating", temp_cfg.get("enable_heating"))
    temp_cfg.setdefault("last_logged_enable_cooling", temp_cfg.get("enable_cooling"))
    # New flag to turn on/off the entire temp-control UI and behavior:
    temp_cfg.setdefault("temp_control_enabled", True)
    # New flag to control active monitoring/recording (user-controlled switch):
    temp_cfg.setdefault("temp_control_active", False)
    # Trigger states for event-based logging:
    temp_cfg.setdefault("in_range_trigger_armed", True)
    temp_cfg.setdefault("above_limit_trigger_armed", True)
    temp_cfg.setdefault("below_limit_trigger_armed", True)

ensure_temp_defaults()

def ensure_all_tilts():
    try:
        for color in TILT_UUIDS.values():
            if color not in tilt_cfg:
                tilt_cfg[color] = {
                    "beer_name": "",
                    "batch_name": "",
                    "ferm_start_date": "",
                    "recipe_og": "",
                    "recipe_fg": "",
                    "recipe_abv": "",
                    "actual_og": None,
                    "brewid": "",
                    "og_confirmed": False
                }
    except Exception:
        pass

ensure_all_tilts()

_TZ_ABBREV_MAP = {
    'EST': 'America/New_York',
    'EDT': 'America/New_York',
    'CST': 'America/Chicago',
    'CDT': 'America/Chicago',
    'MST': 'America/Denver',
    'MDT': 'America/Denver',
    'PST': 'America/Los_Angeles',
    'PDT': 'America/Los_Angeles',
    'UTC': 'UTC'
}
tz = (system_cfg.get('timezone') if isinstance(system_cfg, dict) else None) or os.environ.get('TZ') or 'UTC'
tz = _TZ_ABBREV_MAP.get(tz, tz)
os.environ['TZ'] = tz
try:
    time.tzset()
except Exception:
    pass

# --- Inter-process queues and kasa worker startup --------------------------
# Set multiprocessing start method to 'fork' for better network access in worker
# The 'fork' method preserves the network stack and environment from parent process
# This is critical for KASA plug communication in the worker process
try:
    # Check if 'fork' is available on this platform (not available on Windows)
    available_methods = multiprocessing.get_all_start_methods()
    if 'fork' in available_methods:
        # Only set if not already set (avoid errors if called multiple times)
        if multiprocessing.get_start_method(allow_none=True) is None:
            multiprocessing.set_start_method('fork')
            print("[LOG] Set multiprocessing start method to 'fork' for network access")
        else:
            current = multiprocessing.get_start_method()
            print(f"[LOG] Multiprocessing start method already set to: {current}")
            if current != 'fork':
                print(f"[LOG] WARNING: Using '{current}' instead of 'fork' - may affect KASA plug reliability")
    else:
        print(f"[LOG] WARNING: 'fork' method not available on this platform ({sys.platform})")
        print(f"[LOG] Available methods: {available_methods}")
        print(f"[LOG] KASA plug control may experience network issues")
except RuntimeError as e:
    print(f"[LOG] Could not set multiprocessing start method: {e}")

kasa_queue = Queue()
kasa_result_queue = Queue()
kasa_proc = None

if kasa_worker:
    try:
        kasa_proc = Process(target=kasa_worker, args=(kasa_queue, kasa_result_queue))
        kasa_proc.daemon = True
        kasa_proc.start()
        print("[LOG] Started kasa_worker process")
        # Give the worker process time to initialize before attempting queries
        # This prevents race conditions where sync_plug_states_at_startup() runs
        # before the worker is ready to process commands
        time.sleep(2)
        if kasa_proc.is_alive():
            print("[LOG] kasa_worker process is running and ready")
        else:
            print("[LOG] WARNING: kasa_worker process failed to start properly")
    except Exception as e:
        print("[LOG] Could not start kasa_worker:", e)
else:
    print("[LOG] kasa_worker not available — plug control disabled")

# --- Live runtime data -----------------------------------------------------
live_tilts = {}
tilt_status = {}

last_tilt_log_ts = {}
batch_notification_state = {}  # Track notification state per tilt/brewid

# Notification timing constants
DAILY_REPORT_COOLDOWN_HOURS = 23  # Prevent duplicate daily reports (allows timing variance)
DAILY_REPORT_WINDOW_MINUTES = 5   # Time window for daily report triggering
BATCH_MONITORING_INTERVAL_SECONDS = 300  # Check signal loss and daily reports every 5 minutes

# Notification retry constants (Option A: Auto-retry with exponential backoff)
NOTIFICATION_MAX_RETRIES = 2  # Maximum retry attempts (total: 1 initial + 2 retries = 3 attempts)
NOTIFICATION_RETRY_INTERVALS = [300, 1800]  # Retry after 5 minutes, then 30 minutes (in seconds)
notification_retry_queue = []  # Queue of failed notifications pending retry

# Pending notification queue (for deduplication)
NOTIFICATION_PENDING_DELAY_SECONDS = 10  # Delay before sending to allow deduplication
pending_notifications = []  # Queue of notifications pending send with 10-second delay

def generate_brewid(beer_name, batch_name, date_str):
    id_str = f"{beer_name}-{batch_name}-{date_str}"
    return hashlib.sha256(id_str.encode('utf-8')).hexdigest()[:8]

def update_live_tilt(color, gravity, temp_f, rssi):
    cfg = tilt_cfg.get(color, {})
    live_tilts[color] = {
        "gravity": round(gravity, 3) if gravity is not None else None,
        "temp_f": temp_f,
        "rssi": rssi,
        "timestamp": datetime.utcnow().replace(microsecond=0).isoformat() + "Z",
        "color_code": COLOR_MAP.get(color, "#333"),
        "beer_name": cfg.get("beer_name", ""),
        "batch_name": cfg.get("batch_name", ""),
        "brewid": cfg.get("brewid", ""),
        "recipe_og": cfg.get("recipe_og", ""),
        "recipe_fg": cfg.get("recipe_fg", ""),
        "recipe_abv": cfg.get("recipe_abv", ""),
        "actual_og": cfg.get("actual_og", ""),
        "og_confirmed": cfg.get("og_confirmed", False),
        "original_gravity": cfg.get("actual_og", 0),
    }

def get_active_tilts():
    """
    Filter live_tilts to only include tilts that have sent data recently.
    
    Returns:
        dict: Dictionary of active tilts (those within the inactivity timeout)
    """
    # Get timeout from system config, default to 30 minutes
    timeout_minutes = int(system_cfg.get('tilt_inactivity_timeout_minutes', 30))
    now = datetime.utcnow()
    active_tilts = {}
    
    for color, info in live_tilts.items():
        timestamp_str = info.get('timestamp')
        if not timestamp_str:
            # No timestamp means we can't determine activity - exclude for safety
            continue
        
        try:
            # Parse ISO 8601 timestamp (remove 'Z' suffix for naive UTC datetime)
            # This is consistent with how timestamps are created: datetime.utcnow().isoformat() + "Z"
            timestamp = datetime.fromisoformat(timestamp_str.rstrip('Z'))
            
            elapsed_minutes = (now - timestamp).total_seconds() / 60.0
            
            if elapsed_minutes < timeout_minutes:
                active_tilts[color] = info
        except Exception as e:
            # Unable to parse timestamp - likely corrupted data, exclude from display
            print(f"[LOG] Error parsing timestamp for {color}: {e}, excluding from active tilts")
    
    return active_tilts

def get_control_tilt_color():
    """
    Get the color of the Tilt currently being used for temperature control.
    
    Important behavior:
    - If a Tilt is explicitly assigned (tilt_color is set), ALWAYS return that color,
      even if the Tilt is currently offline or inactive. This ensures safety shutdown
      triggers when the assigned Tilt is not operational.
    - If no Tilt is assigned (tilt_color is empty), use fallback logic to find any
      available Tilt with temperature data.
    
    Returns:
        str: The color of the Tilt being used, or None if no Tilt is being used.
    """
    # First check if a Tilt is explicitly assigned
    color = temp_cfg.get("tilt_color")
    if color:
        # If a Tilt is explicitly assigned, ALWAYS return it
        # Even if it's not in live_tilts (offline/inactive)
        # This ensures safety shutdown triggers when assigned Tilt is not operational
        # We do NOT fall back to another Tilt in this case
        return color
    
    # If no explicit assignment (tilt_color is empty), check if we're using a fallback Tilt
    # This happens when tilt_color is empty but we still get temp from a Tilt
    for tilt_color, info in live_tilts.items():
        if info.get("temp_f") is not None:
            return tilt_color
    
    return None

def get_current_temp_for_control_tilt():
    """
    Get the current temperature from the Tilt assigned to temperature control.
    
    Important behavior:
    - If a Tilt is explicitly assigned, ONLY use that Tilt's temperature.
    - Do NOT fall back to another Tilt if the assigned one is offline/inactive.
    - If no Tilt is assigned, use fallback logic to get temp from any available Tilt.
    
    Returns:
        float: Temperature in Fahrenheit, or None if no temperature available.
    """
    color = temp_cfg.get("tilt_color")
    if color:
        # Tilt is explicitly assigned - ONLY use that Tilt
        # Do NOT fall back to another Tilt if this one is offline
        if color in live_tilts:
            return live_tilts[color].get("temp_f")
        else:
            # Assigned Tilt is not available - return None
            # This will trigger safety shutdown
            return None
    
    # No explicit assignment - use fallback logic
    # Return temperature from any available Tilt
    for info in live_tilts.values():
        if info.get("temp_f") is not None:
            return info.get("temp_f")
    return None

def is_control_tilt_active():
    """
    Check if the Tilt being used for temperature control is currently active.
    
    For temperature control safety, uses a shorter timeout than general Tilt monitoring:
    - Temperature control timeout: 2 × update_interval (default: 2 × 2 min = 4 minutes)
    - This ensures KASA plugs turn off quickly if Tilt signal is lost
    - Much shorter than the general 30-minute inactivity timeout used for display/notifications
    
    Grace period for newly assigned Tilts:
    - When a Tilt is first assigned to temp control, there's a 15-minute grace period
    - During this grace period, the system allows time for the Tilt to start broadcasting
    - This prevents immediate shutdown when setting up a new batch
    - After grace period, normal 4-minute timeout applies
    
    This includes both explicitly assigned Tilts (via tilt_color setting) and
    fallback Tilts (when tilt_color is empty but temperature is sourced from a Tilt).
    
    Returns:
        bool: True if the control Tilt is active (within temp control timeout) OR if no Tilt is being used.
              False only if a Tilt is being used for control but is inactive (safety shutdown condition).
    """
    # Get the color of the Tilt actually being used for control
    control_color = get_control_tilt_color()
    
    if not control_color:
        # No Tilt is being used for temp control - allow control to proceed
        # (temperature might be set manually)
        return True
    
    # Check if we're in the grace period for a newly assigned Tilt
    # Grace period: 15 minutes from when Tilt was assigned to temp control
    assignment_timestamp = temp_cfg.get("tilt_assignment_time")
    if assignment_timestamp:
        try:
            from datetime import datetime
            assignment_time = datetime.fromisoformat(assignment_timestamp)
            now = datetime.utcnow()
            minutes_since_assignment = (now - assignment_time).total_seconds() / 60.0
            
            # Grace period: 15 minutes
            grace_period_minutes = 15
            
            if minutes_since_assignment < grace_period_minutes:
                # We're in grace period - allow control to proceed even if Tilt is inactive
                # This gives time for Tilt to start broadcasting and for user to complete setup
                print(f"[TEMP_CONTROL] Grace period active: {minutes_since_assignment:.1f}/{grace_period_minutes} minutes elapsed")
                return True
        except Exception as e:
            print(f"[LOG] Error checking assignment time: {e}")
            # If we can't parse the assignment time, continue with normal checks
    
    # For temperature control, use a much shorter timeout than general monitoring
    # Timeout = 2 × update_interval (2 missed readings)
    # Example: 2 min update interval → 4 min timeout
    try:
        update_interval_minutes = int(system_cfg.get("update_interval", 2))
    except Exception:
        update_interval_minutes = 2
    
    # Temperature control timeout: 2 missed readings
    temp_control_timeout_minutes = update_interval_minutes * 2
    
    # Check if the control Tilt has sent data within the temp control timeout
    if control_color not in live_tilts:
        return False
    
    tilt_info = live_tilts[control_color]
    timestamp_str = tilt_info.get('timestamp')
    if not timestamp_str:
        return False
    
    try:
        from datetime import datetime
        timestamp = datetime.fromisoformat(timestamp_str.rstrip('Z'))
        now = datetime.utcnow()
        elapsed_minutes = (now - timestamp).total_seconds() / 60.0
        
        # Tilt is active if it's within the temp control timeout
        return elapsed_minutes < temp_control_timeout_minutes
    except Exception as e:
        print(f"[LOG] Error checking control Tilt activity for {control_color}: {e}")
        # If we can't determine activity, assume inactive for safety
        return False

def log_tilt_reading(color, gravity, temp_f, rssi):
    """
    Log tilt readings with interval-based rate limiting and batch tracking.
    
    This function handles:
    - Rate-limited logging based on tilt_logging_interval_minutes
    - Recording readings to control log and batch-specific JSONL files
    - Forwarding to third-party services if configured
    - Triggering batch notifications (signal loss, fermentation start, etc.)
    
    Args:
        color: Tilt color identifier
        gravity: Specific gravity reading
        temp_f: Temperature in Fahrenheit
        rssi: Bluetooth signal strength
    """
    cfg = tilt_cfg.get(color, {})
    brewid = cfg.get('brewid', '')
    
    # Rate limiting based on tilt_logging_interval_minutes
    interval_minutes = int(system_cfg.get('tilt_logging_interval_minutes', 15))
    now = datetime.utcnow()
    last_log = last_tilt_log_ts.get(color)
    
    if last_log:
        elapsed = (now - last_log).total_seconds() / 60.0
        if elapsed < interval_minutes:
            return
    
    last_tilt_log_ts[color] = now
    
    # Create payload
    payload = {
        "timestamp": now.replace(microsecond=0).isoformat() + "Z",
        "tilt_color": color,
        "gravity": round(gravity, 3) if gravity is not None else None,
        "temp_f": temp_f,
        "rssi": rssi,
        "beer_name": cfg.get("beer_name", ""),
        "batch_name": cfg.get("batch_name", ""),
        "brewid": brewid,
        "recipe_og": cfg.get("recipe_og", ""),
        "actual_og": cfg.get("actual_og"),
        "og_confirmed": cfg.get("og_confirmed", False)
    }
    
    # Log to control log
    append_control_log("tilt_reading", payload)
    
    # Log to batch-specific jsonl
    if brewid:
        append_sample_to_batch_jsonl(color, brewid, payload)
    
    # Forward to third-party if configured
    forward_to_third_party_if_configured(payload)
    
    # Track batch notification state and check triggers
    check_batch_notifications(color, gravity, temp_f, brewid, cfg)

def detection_callback(device, advertisement_data):
    try:
        mfg_data = advertisement_data.manufacturer_data
        if not mfg_data:
            return
        raw = list(mfg_data.values())[0]
        if len(raw) < 22:
            return
        uuid = raw[2:18].hex()
        color = TILT_UUIDS.get(uuid) or TILT_UUIDS.get(uuid.lower()) or TILT_UUIDS.get(uuid.upper())
        if not color:
            return
        try:
            temp_f = int.from_bytes(raw[18:20], byteorder='big')
            gravity = int.from_bytes(raw[20:22], byteorder='big') / 1000.0
        except Exception:
            return
        rssi = advertisement_data.rssi
        update_live_tilt(color, gravity, temp_f, rssi)
        try:
            log_tilt_reading(color, gravity, temp_f, rssi)
        except Exception as log_err:
            print(f"[BLE] log_tilt_reading failed for {color}: {log_err}")
    except Exception as e:
        print("[BLE] detection_callback exception:", e)

# --- Batch rotation / archival (legacy, kept for compatibility) ------------
def rotate_and_archive_old_history(color, old_brewid, old_cfg):
    try:
        if not old_brewid and not color:
            return False
        os.makedirs(BATCHES_DIR, exist_ok=True)
        archive_name = f"{color}_{old_cfg.get('beer_name','unknown')}_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.jsonl"
        safe_archive = os.path.join(BATCHES_DIR, archive_name.replace(' ', '_'))
        moved = 0
        remaining_lines = []
        if os.path.exists(LOG_PATH):
            with open(LOG_PATH, 'r') as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                    except Exception:
                        remaining_lines.append(line)
                        continue
                    if obj.get('event') != 'SAMPLE':
                        remaining_lines.append(line)
                        continue
                    payload = obj or {}
                    if isinstance(payload, dict) and payload.get('brewid') == old_brewid:
                        with open(safe_archive, 'a') as af:
                            af.write(json.dumps(obj) + "\n")
                        moved += 1
                    else:
                        remaining_lines.append(line)
        try:
            with open(LOG_PATH, 'w') as f:
                f.writelines(remaining_lines)
        except Exception as e:
            print(f"[LOG] Error rewriting main log after archive: {e}")

        # Only log mode change if we actually archived samples
        if moved > 0:
            append_control_log("temp_control_mode_changed", {"tilt_color": color, "low_limit": temp_cfg.get("low_limit"), "current_temp": temp_cfg.get("current_temp"), "high_limit": temp_cfg.get("high_limit")})
        return True
    except Exception as e:
        print(f"[LOG] rotate_and_archive_old_history error: {e}")
        return False

# --- batches: per-batch jsonl helpers --------------------------------------
def ensure_batches_dir():
    try:
        os.makedirs(BATCHES_DIR, exist_ok=True)
    except Exception as e:
        print(f"[LOG] Could not create batches dir {BATCHES_DIR}: {e}")

def normalize_to_mmddyyyy(date_str):
    if not date_str:
        return datetime.utcnow().strftime("%m%d%Y")
    for fmt in ("%m-%d-%Y", "%m/%d/%Y", "%Y-%m-%d", "%Y/%m/%d", "%m%d%Y"):
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%m%d%Y")
        except Exception:
            continue
    try:
        dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return dt.strftime("%m%d%Y")
    except Exception:
        return datetime.utcnow().strftime("%m%d%Y")

def normalize_to_yyyymmdd(date_str):
    """Convert various date formats to YYYYmmdd format."""
    if not date_str:
        return datetime.utcnow().strftime("%Y%m%d")
    for fmt in ("%m-%d-%Y", "%m/%d/%Y", "%Y-%m-%d", "%Y/%m/%d", "%m%d%Y", "%Y%m%d"):
        try:
            dt = datetime.strptime(date_str, fmt)
            return dt.strftime("%Y%m%d")
        except Exception:
            continue
    try:
        dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
        return dt.strftime("%Y%m%d")
    except Exception:
        return datetime.utcnow().strftime("%Y%m%d")

def sanitize_filename(name):
    """Sanitize a string for use in a filename."""
    # Replace invalid characters with underscore
    invalid_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|', '\n', '\r', '\t']
    result = name
    for char in invalid_chars:
        result = result.replace(char, '_')
    # Also replace spaces and remove control characters
    result = result.replace(' ', '_')
    # Remove ASCII control characters (0-31)
    result = ''.join(c if ord(c) >= 32 else '_' for c in result)
    # Limit length to avoid overly long filenames
    return result[:MAX_FILENAME_LENGTH]

def batch_jsonl_filename(color, brewid, created_date_mmddyyyy=None, beer_name=None, batch_name=None):
    """Generate batch JSONL filename in format: brewname_YYYYmmdd_brewid.jsonl
    
    First searches for an existing file containing the brewid.
    If found, returns that file to prevent multiple files for the same batch.
    If not found, generates a new filename.
    """
    ensure_batches_dir()
    bid = (brewid or "unknown")
    
    # First, search for any existing file that contains this brewid
    # Match brewid as complete token: either whole filename or preceded by underscore
    try:
        for fn in os.listdir(BATCHES_DIR):
            if not fn.endswith('.jsonl'):
                continue
            # Remove .jsonl extension for matching
            name_without_ext = fn.removesuffix('.jsonl')
            # Match if brewid is the entire name, or ends with _brewid
            # This ensures exact token matching: "abc" matches "abc.jsonl" or "name_abc.jsonl"
            # but NOT "xyzabc.jsonl" (no underscore separator before brewid)
            if name_without_ext == bid or name_without_ext.endswith(f"_{bid}"):
                # Found an existing file with this brewid
                existing_path = os.path.join(BATCHES_DIR, fn)
                print(f"[BATCH] Found existing batch file for brewid {bid}: {fn}")
                return existing_path
    except Exception as e:
        print(f"[BATCH] Error searching for existing batch file: {e}")
    
    # No existing file found, generate a new filename
    # Get beer_name from tilt config if not provided
    if beer_name is None:
        cfg = tilt_cfg.get(color, {})
        beer_name = cfg.get("beer_name", "")
    
    # Create filename with brew name, date, and brewid
    if beer_name:
        safe_beer_name = sanitize_filename(beer_name)
    else:
        safe_beer_name = "Batch"
    
    # Convert date to YYYYmmdd format
    if created_date_mmddyyyy:
        date_yyyymmdd = normalize_to_yyyymmdd(created_date_mmddyyyy)
    else:
        date_yyyymmdd = datetime.utcnow().strftime("%Y%m%d")
    
    fname = f"{safe_beer_name}_{date_yyyymmdd}_{bid}.jsonl"
    print(f"[BATCH] Creating new batch file for brewid {bid}: {fname}")
    return os.path.join(BATCHES_DIR, fname)

def ensure_batch_jsonl_exists(color, brewid, meta=None, created_date_mmddyyyy=None):
    beer_name = None
    if meta and isinstance(meta, dict):
        beer_name = meta.get("beer_name", "")
    if not beer_name:
        cfg = tilt_cfg.get(color, {})
        beer_name = cfg.get("beer_name", "")
    
    path = batch_jsonl_filename(color, brewid, created_date_mmddyyyy=created_date_mmddyyyy, beer_name=beer_name)
    if not os.path.exists(path):
        # Try to migrate legacy files (both old formats)
        try:
            # Try pattern 1: batch_{COLOR}_{brewid}_
            legacy_pattern1 = f"batch_{(color or '').upper()}_{brewid}_"
            # Try pattern 2: {brewid}.jsonl
            legacy_pattern2 = f"{brewid}.jsonl"
            
            migrated = False
            for fn in os.listdir(BATCHES_DIR):
                if migrated:
                    break
                if fn.startswith(legacy_pattern1) or fn == legacy_pattern2:
                    legacy_path = os.path.join(BATCHES_DIR, fn)
                    try:
                        os.rename(legacy_path, path)
                        print(f"[MIGRATE] Renamed legacy {legacy_path} -> {path}")
                        migrated = True
                    except Exception as e:
                        print(f"[MIGRATE] Could not rename {legacy_path} -> {path}: {e}")
        except Exception as e:
            print(f"[MIGRATE] Migration scan failed: {e}")
        try:
            header = {
                "event": "batch_metadata",
                "payload": {
                    "tilt_color": color,
                    "brewid": brewid,
                    "created_date": (created_date_mmddyyyy or datetime.utcnow().strftime("%m%d%Y")),
                    "meta": meta or {}
                }
            }
            with open(path, "a", encoding="utf-8") as f:
                f.write(json.dumps(header) + "\n")
        except Exception as e:
            print(f"[LOG] Could not create batch jsonl {path}: {e}")
    return path

def append_sample_to_batch_jsonl(color, brewid, sample_payload, created_date_mmddyyyy=None):
    cfg = tilt_cfg.get(color, {})
    beer_name = cfg.get("beer_name", "")
    path = batch_jsonl_filename(color, brewid, created_date_mmddyyyy=created_date_mmddyyyy, beer_name=beer_name)
    try:
        if not os.path.exists(path):
            ensure_batch_jsonl_exists(color, brewid, meta={"beer_name": beer_name, "batch_name": cfg.get("batch_name", "")}, created_date_mmddyyyy=created_date_mmddyyyy)
        entry = {"event": "sample", "payload": sample_payload}
        with open(path, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry) + "\n")
        return True
    except Exception as e:
        print(f"[LOG] append_sample_to_batch_jsonl failed for {color}/{brewid}: {e}")
        return False

def write_normalized_tilt_reading(payload, event_name="tilt_reading"):
    try:
        entry = {"event": event_name, "payload": payload}
        dirname = os.path.dirname(LOG_PATH)
        if dirname and not os.path.exists(dirname):
            os.makedirs(dirname, exist_ok=True)
        with open(LOG_PATH, "a", encoding="utf-8") as f:
            f.write(json.dumps(entry) + "\n")
        return True
    except Exception as e:
        print(f"[LOG] write_normalized_tilt_reading failed: {e}")
        return False

def forward_to_third_party_if_configured(payload):
    """
    Forward tilt reading data to configured external services.
    
    Supports two configuration methods:
    1. Per-tilt external_url in tilt_cfg[color] (highest priority)
    2. System-wide external_url_0, external_url_1, external_url_2 in system_cfg
    
    The function will try to send to all configured URLs.
    Automatically transforms the payload to Brewers Friend format if URL contains "brewersfriend.com".
    """
    color = (payload.get("tilt_color") or "").upper()
    if not color:
        return {"forwarded": False, "reason": "no color"}
    
    if requests is None:
        return {"forwarded": False, "reason": "requests library not available"}
    
    # Collect all URLs to forward to
    urls_to_forward = []
    
    # 1. Check per-tilt configuration
    tc = tilt_cfg.get(color) or {}
    tilt_url = tc.get("external_url")
    if tilt_url:
        urls_to_forward.append({
            "url": tilt_url,
            "method": (tc.get("external_method") or "POST").upper(),
            "send_json": bool(tc.get("external_json")) if ("external_json" in tc) else True
        })
    
    # 2. Check system-wide configuration
    # Support new format (external_urls array) and old format (external_url_0, etc.) for backwards compatibility
    external_urls = system_cfg.get("external_urls", [])
    
    if external_urls and isinstance(external_urls, list):
        # New format: per-URL configuration
        for url_config in external_urls:
            if not isinstance(url_config, dict):
                continue
            url = url_config.get("url", "").strip()
            if not url:
                continue
            
            # Get field map if specified
            field_map_id = url_config.get("field_map_id", "default")
            predefined_maps = get_predefined_field_maps()
            field_map = predefined_maps.get(field_map_id, {}).get("map")
            
            # Handle custom field map JSON
            if field_map_id == "custom" and url_config.get("custom_field_map"):
                try:
                    field_map = json.loads(url_config["custom_field_map"])
                except (json.JSONDecodeError, ValueError, TypeError):
                    field_map = None
            
            urls_to_forward.append({
                "url": url,
                "method": url_config.get("method", "POST").upper(),
                "send_json": (url_config.get("content_type", "form") == "json"),
                "timeout": int(url_config.get("timeout_seconds", 8)),
                "field_map": field_map
            })
    else:
        # Old format: external_url_0, external_url_1, external_url_2 with shared settings
        for i in range(3):
            sys_url = system_cfg.get(f"external_url_{i}")
            if sys_url:
                # Parse old external_field_map if present
                field_map = None
                if system_cfg.get("external_field_map"):
                    try:
                        field_map = json.loads(system_cfg["external_field_map"])
                    except (json.JSONDecodeError, ValueError, TypeError):
                        pass
                
                urls_to_forward.append({
                    "url": sys_url,
                    "method": system_cfg.get("external_method", "POST").upper(),
                    "send_json": (system_cfg.get("external_content_type", "form") == "json"),
                    "timeout": int(system_cfg.get("external_timeout_seconds", 8)),
                    "field_map": field_map
                })
    
    if not urls_to_forward:
        return {"forwarded": False, "reason": "no external_url configured"}
    
    # Forward to all configured URLs
    results = []
    for config in urls_to_forward:
        url = config["url"]
        method = config["method"]
        send_json = config["send_json"]
        timeout = config.get("timeout", 8)
        field_map = config.get("field_map")
        
        # Transform payload for Brewers Friend if needed (uses original payload)
        if "brewersfriend.com" in url.lower():
            # Brewers Friend expects a specific format with numeric values
            transformed_payload = {
                "name": payload.get("tilt_color", "Tilt"),
                "temp": payload.get("temp_f") if payload.get("temp_f") is not None else 0,
                "temp_unit": "F",
                "gravity": payload.get("gravity") if payload.get("gravity") is not None else 0,
                "gravity_unit": "G",
                "beer": payload.get("beer_name", "") or payload.get("batch_name", ""),
                "comment": f"Batch: {payload.get('batch_name', '')} | BrewID: {payload.get('brewid', '')}"
            }
            forwarding_payload = transformed_payload
            # Brewers Friend always uses JSON
            send_json = True
        elif field_map:
            # Apply field map transformation if provided and not Brewers Friend
            forwarding_payload = {}
            for logical_field, remote_field in field_map.items():
                if logical_field in payload:
                    forwarding_payload[remote_field] = payload[logical_field]
        else:
            # Use original payload if no transformation needed
            forwarding_payload = payload
        
        headers = {}
        try:
            if send_json:
                headers["Content-Type"] = "application/json"
                resp = requests.request(method, url, json=forwarding_payload, headers=headers, timeout=timeout)
            else:
                headers["Content-Type"] = "application/x-www-form-urlencoded"
                formdata = {k: ("" if v is None else v) for k, v in forwarding_payload.items() if isinstance(v, (str, int, float)) or v is None}
                resp = requests.request(method, url, data=formdata, headers=headers, timeout=timeout)
            
            result = {"url": url, "forwarded": True, "status_code": resp.status_code, "text": resp.text[:500]}
            results.append(result)
            print(f"[FORWARD] Successfully forwarded tilt {color} to {url}, status: {resp.status_code}")
        except Exception as e:
            result = {"url": url, "forwarded": False, "error": str(e)}
            results.append(result)
            print(f"[FORWARD] Error forwarding tilt {color} to {url}: {e}")
    
    # Return summary
    success_count = sum(1 for r in results if r.get("forwarded"))
    return {
        "forwarded": success_count > 0,
        "success_count": success_count,
        "total_count": len(results),
        "results": results
    }

# --- Notifications helpers -------------------------------------------------
def _smtp_send(recipient, subject, body):
    cfg = system_cfg
    sending_email = cfg.get("sending_email") or cfg.get("email")
    if not (isinstance(cfg, dict) and sending_email):
        error_msg = "SMTP configuration incomplete: sender email not configured"
        print(f"[LOG] {error_msg}")
        return False, error_msg
    try:
        msg = MIMEText(body)
        msg["Subject"] = subject
        msg["From"] = sending_email
        msg["To"] = recipient
        server = smtplib.SMTP(cfg.get("smtp_host", "localhost"), int(cfg.get("smtp_port", 25)), timeout=10)
        if cfg.get("smtp_starttls"):
            server.starttls()
        # Use sending_email as username and smtp_password (or sending_email_password) for authentication
        smtp_password = cfg.get("smtp_password") or cfg.get("sending_email_password")
        if sending_email and smtp_password and len(smtp_password) > 0:
            server.login(sending_email, smtp_password)
        server.sendmail(sending_email, [recipient], msg.as_string())
        server.quit()
        return True, "Success"
    except Exception as e:
        original_error = str(e)
        print(f"[LOG] SMTP send failed: {original_error}")
        
        # Provide helpful error message for Gmail authentication issues
        if "BadCredentials" in original_error or ("535" in original_error and "gmail" in cfg.get("smtp_host", "").lower()):
            error_msg = (
                "Gmail authentication failed. Gmail requires an App Password when 2-Factor Authentication is enabled. "
                "Regular Gmail passwords will not work. "
                "To fix this: 1) Enable 2-Factor Authentication on your Google account, "
                "2) Generate an App Password at https://myaccount.google.com/apppasswords, "
                "3) Use the App Password (not your regular password) in the Fermenter Email Password field. "
                f"Original error: {original_error}"
            )
        else:
            error_msg = original_error
        
        return False, error_msg

def send_email(subject, body):
    recipient = system_cfg.get("email")
    if not recipient:
        print("[LOG] No recipient email configured")
        temp_cfg["email_error"] = True
        return False, "No recipient email configured"
    success, error_msg = _smtp_send(recipient, subject, body)
    temp_cfg["email_error"] = not success
    return success, error_msg

def send_push(body, subject="Fermenter Notification"):
    """
    Send push notification using configured push provider.
    
    Supported providers:
    - Pushover (paid, $5 one-time per platform, very reliable)
    - ntfy (free, open-source, self-hostable)
    """
    push_provider = system_cfg.get("push_provider", "pushover").lower()
    
    if push_provider == "ntfy":
        return _send_push_ntfy(body, subject)
    else:  # Default to Pushover
        return _send_push_pushover(body, subject)

def _send_push_pushover(body, subject="Fermenter Notification"):
    """Send push notification using Pushover API"""
    if not requests:
        error_msg = "requests library not installed. Run: pip install requests"
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg
    
    # Get Pushover credentials from config
    user_key = system_cfg.get("pushover_user_key", "").strip()
    api_token = system_cfg.get("pushover_api_token", "").strip()
    
    # Validate configuration
    if not user_key or not api_token:
        error_msg = "Pushover User Key and API Token must be configured in System Settings. Sign up at https://pushover.net"
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg
    
    try:
        # Pushover API endpoint
        url = "https://api.pushover.net/1/messages.json"
        
        # Prepare payload
        payload = {
            "token": api_token,
            "user": user_key,
            "title": subject,
            "message": body,
            "priority": 0  # Normal priority
        }
        
        # Optional: Set device if configured
        device = system_cfg.get("pushover_device", "").strip()
        if device:
            payload["device"] = device
        
        # Send push notification
        response = requests.post(url, data=payload, timeout=10)
        
        if response.status_code == 200:
            print(f"[LOG] Pushover push notification sent successfully")
            temp_cfg["push_error"] = False
            return True, "Success"
        else:
            error_msg = f"Pushover returned status {response.status_code}: {response.text[:200]}"
            print(f"[LOG] {error_msg}")
            temp_cfg["push_error"] = True
            return False, error_msg
        
    except Exception as e:
        error_msg = f"Pushover push notification failed: {str(e)}"
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg

def _send_push_ntfy(body, subject="Fermenter Notification"):
    """Send push notification using ntfy (free, open-source)"""
    if not requests:
        error_msg = "requests library not installed. Run: pip install requests"
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg
    
    # Get ntfy configuration
    ntfy_server = system_cfg.get("ntfy_server", "https://ntfy.sh").strip()
    ntfy_topic = system_cfg.get("ntfy_topic", "").strip()
    
    # Validate configuration
    if not ntfy_topic:
        error_msg = "ntfy Topic must be configured. Choose a unique topic name and configure it in System Settings."
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg
    
    try:
        # ntfy API endpoint
        url = f"{ntfy_server}/{ntfy_topic}"
        
        # Prepare headers (ntfy uses headers for metadata)
        headers = {
            "Title": subject,
            "Priority": "default",
            "Tags": "beer,fermentation"
        }
        
        # Optional: Add auth token if configured
        auth_token = system_cfg.get("ntfy_auth_token", "").strip()
        if auth_token:
            headers["Authorization"] = f"Bearer {auth_token}"
        
        # Send push notification (body is sent as plain text in request body)
        response = requests.post(
            url,
            data=body.encode('utf-8'),
            headers=headers,
            timeout=10
        )
        
        if response.status_code == 200:
            print(f"[LOG] ntfy push notification sent successfully")
            temp_cfg["push_error"] = False
            return True, "Success"
        else:
            error_msg = f"ntfy returned status {response.status_code}: {response.text[:200]}"
            print(f"[LOG] {error_msg}")
            temp_cfg["push_error"] = True
            return False, error_msg
            
    except Exception as e:
        error_msg = f"ntfy push notification failed: {str(e)}"
        print(f"[LOG] {error_msg}")
        temp_cfg["push_error"] = True
        return False, error_msg

def attempt_send_notifications(subject, body):
    # Use system_cfg for notification mode
    mode = (system_cfg.get('warning_mode') or 'NONE').upper()
    success_any = False
    temp_cfg['notifications_trigger'] = True
    
    # Reset error flags before attempting
    temp_cfg['push_error'] = False
    temp_cfg['email_error'] = False
    
    try:
        if mode == 'EMAIL':
            success_any, error_msg = send_email(subject, body)
            if not success_any:
                print(f"[LOG] Email notification failed: {error_msg}")
        elif mode == 'PUSH':
            success_any, error_msg = send_push(body, subject)
            if not success_any:
                print(f"[LOG] Push notification failed: {error_msg}")
        elif mode == 'BOTH':
            e, email_error = send_email(subject, body)
            p, push_error = send_push(body, subject)
            if not e:
                print(f"[LOG] Email notification failed: {email_error}")
            if not p:
                print(f"[LOG] Push notification failed: {push_error}")
            success_any = e or p
        else:
            success_any = False
    except Exception as e:
        print(f"[LOG] Notification attempt exception: {e}")
        success_any = False

    temp_cfg['notifications_trigger'] = False
    if success_any:
        temp_cfg['notification_last_sent'] = datetime.utcnow().isoformat()
        temp_cfg['notification_comm_failure'] = False
        return True
    else:
        temp_cfg['notification_comm_failure'] = True
        # Don't disable notifications on failure - just track the failure
        return False

def send_warning(subject, body):
    mode = (system_cfg.get('warning_mode') or 'NONE').upper()
    if mode == 'NONE':
        return False
    try:
        rate_limit = int(system_cfg.get('notification_rate_limit_seconds', 3600))
    except Exception:
        rate_limit = 3600

    last = temp_cfg.get('notification_last_sent')
    if last:
        try:
            last_dt = datetime.fromisoformat(last)
            elapsed = (datetime.utcnow() - last_dt).total_seconds()
            if elapsed < rate_limit:
                return False
        except Exception:
            pass

    temp_cfg['notifications_trigger'] = True
    ok = attempt_send_notifications(subject, body)
    return ok

def send_temp_control_notification(event_type, temp, low_limit, high_limit, tilt_color):
    """
    Send notifications for temperature control events if enabled in settings.
    Uses the pending queue system with deduplication to prevent duplicate alerts.
    
    Handles all temperature control events: temp limits, heating/cooling on/off.
    Users can individually enable/disable each notification type.
    """
    # Get temp control notification settings
    temp_notif_cfg = system_cfg.get('temp_control_notifications', {})
    
    # Check if this specific event type is enabled
    if not temp_notif_cfg.get(f'enable_{event_type}', False):
        return
    
    brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
    now = datetime.utcnow()
    
    # Create caption based on event type
    caption_map = {
        'temp_below_low_limit': f'Temperature Below Low Limit - Current: {temp:.1f}°F, Low Limit: {low_limit:.1f}°F',
        'temp_above_high_limit': f'Temperature Above High Limit - Current: {temp:.1f}°F, High Limit: {high_limit:.1f}°F',
        'heating_on': f'Heating Turned On - Current: {temp:.1f}°F, Low Limit: {low_limit:.1f}°F',
        'heating_off': f'Heating Turned Off - Current: {temp:.1f}°F',
        'cooling_on': f'Cooling Turned On - Current: {temp:.1f}°F, High Limit: {high_limit:.1f}°F',
        'cooling_off': f'Cooling Turned Off - Current: {temp:.1f}°F',
    }
    
    caption = caption_map.get(event_type, f'Temperature Control Event: {event_type}')
    
    subject = f"{brewery_name} - Temperature Control Alert"
    body = f"""Brewery Name: {brewery_name}
Date: {now.strftime('%Y-%m-%d')}
Time: {now.strftime('%H:%M:%S')}
Tilt Color: {tilt_color}

{caption}"""
    
    # Queue notification with 10-second delay for deduplication
    # Use tilt_color as brewid since temp control is per-tilt
    queue_pending_notification(
        notification_type=event_type,
        subject=subject,
        body=body,
        brewid=tilt_color,  # Use tilt_color as identifier for temp control
        color=tilt_color
    )

def send_safety_shutdown_notification(tilt_color, low_limit, high_limit):
    """
    Send notification when control Tilt becomes inactive and safety shutdown is triggered.
    Uses the pending queue system with deduplication to prevent duplicate alerts.
    
    Args:
        tilt_color: The color of the Tilt that went offline
        low_limit: Current low temperature limit
        high_limit: Current high temperature limit
    """
    # Get temp control notification settings
    temp_notif_cfg = system_cfg.get('temp_control_notifications', {})
    
    # Check if safety shutdown notifications are enabled (default to True for safety)
    if not temp_notif_cfg.get('enable_safety_shutdown', True):
        return
    
    brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
    timeout_minutes = int(system_cfg.get('tilt_inactivity_timeout_minutes', 30))
    now = datetime.utcnow()
    
    subject = f"{brewery_name} - SAFETY SHUTDOWN: Control Tilt Offline"
    body = f"""CRITICAL SAFETY ALERT

Brewery Name: {brewery_name}
Date: {now.strftime('%Y-%m-%d')}
Time: {now.strftime('%H:%M:%S')}
Tilt Color: {tilt_color}

SAFETY SHUTDOWN TRIGGERED

The Tilt assigned to temperature control has not transmitted data for more than {timeout_minutes} minutes.

All Kasa plugs have been automatically turned OFF to prevent runaway heating/cooling.

Current Settings:
- Low Limit: {low_limit:.1f}°F
- High Limit: {high_limit:.1f}°F

Action Required:
1. Check Tilt battery
2. Verify Tilt is in range
3. Confirm Bluetooth connectivity
4. Temperature control will resume automatically when Tilt starts transmitting"""
    
    # Queue notification with 10-second delay for deduplication
    queue_pending_notification(
        notification_type='safety_shutdown',
        subject=subject,
        body=body,
        brewid=tilt_color,
        color=tilt_color
    )

def send_kasa_error_notification(mode, url, error_msg):
    """
    Send notifications for Kasa plug connection failures if enabled in settings.
    Uses the pending queue system with deduplication to prevent duplicate alerts.
    Only sends ONE notification per continuous failure period (resets when plug works again).
    
    Args:
        mode: 'heating' or 'cooling'
        url: IP address or hostname of the Kasa plug
        error_msg: Error message from the connection failure
        
    Note:
        This function accesses temp_cfg which is shared across threads. While there's a 
        theoretical race condition, the impact is minimal: worst case is a duplicate 
        notification might be queued (which the pending queue will deduplicate anyway).
        The alternative of adding locks would add complexity and potential deadlocks.
    """
    # Get temp control notification settings
    temp_notif_cfg = system_cfg.get('temp_control_notifications', {})
    
    # Check if Kasa error notifications are enabled
    if not temp_notif_cfg.get('enable_kasa_error', True):
        return
    
    # Check if we've already notified about this error
    # This prevents repeated notifications for the same continuous failure
    notified_flag = f"{mode}_error_notified"
    if temp_cfg.get(notified_flag, False):
        # Already notified about this error, don't send again
        return
    
    # Set the notified flag to prevent duplicate notifications
    # This flag is reset in kasa_result_listener when the plug starts working again
    temp_cfg[notified_flag] = True
    
    brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
    tilt_color = temp_cfg.get('tilt_color', '')
    now = datetime.utcnow()
    
    mode_name = 'Heating' if mode == 'heating' else 'Cooling'
    
    subject = f"{brewery_name} - Kasa Plug Connection Failure"
    body = f"""Brewery Name: {brewery_name}
Date: {now.strftime('%Y-%m-%d')}
Time: {now.strftime('%H:%M:%S')}
Tilt Color: {tilt_color}
Mode: {mode_name}
Plug URL: {url}

Failed to connect to Kasa plug.
Error: {error_msg}"""
    
    # Queue notification with 10-second delay for deduplication
    # Use combination of mode and url as unique identifier
    queue_pending_notification(
        notification_type='kasa_error',
        subject=subject,
        body=body,
        brewid=f"{mode}_{url}",  # Unique identifier for this specific plug and mode
        color=tilt_color
    )

def save_notification_state_to_config(color, brewid):
    """
    Save notification state flags to tilt_config.json for persistence across restarts.
    Only saves the notification flags, not transient data like gravity_history.
    Signal loss is NOT persisted - it resets on restart as requested.
    """
    if brewid not in batch_notification_state:
        return
    
    if not color:
        print(f"[LOG] save_notification_state_to_config: No color provided for brewid {brewid}")
        return
    
    state = batch_notification_state[brewid]
    if color not in tilt_cfg:
        print(f"[LOG] save_notification_state_to_config: Color {color} not in tilt_cfg")
        return
    
    # Only persist notification timestamps, not signal loss (resets on restart)
    tilt_cfg[color]['notification_state'] = {
        'fermentation_start_datetime': state.get('fermentation_start_datetime'),
        'fermentation_completion_datetime': state.get('fermentation_completion_datetime'),
        'last_daily_report': state.get('last_daily_report')
    }
    
    try:
        save_json(TILT_CONFIG_FILE, tilt_cfg)
    except Exception as e:
        print(f"[LOG] Could not save notification state for {color}: {e}")

def load_notification_state_from_config(color, brewid, cfg):
    """
    Load persisted notification state from tilt_config.json.
    Returns a dict with notification state flags.
    Signal loss flags are NOT loaded - they always start fresh on restart.
    """
    persisted_state = cfg.get('notification_state', {})
    
    # Load persisted datetime values for fermentation start/completion
    # Signal loss flags are intentionally NOT persisted (reset on restart)
    return {
        'last_reading_time': datetime.utcnow(),
        'signal_lost': False,  # Always start fresh on restart
        'signal_loss_notified': False,  # Always start fresh on restart
        'fermentation_started': bool(persisted_state.get('fermentation_start_datetime')),
        'fermentation_start_datetime': persisted_state.get('fermentation_start_datetime'),
        'fermentation_completion_datetime': persisted_state.get('fermentation_completion_datetime'),
        'gravity_history': [],
        'last_daily_report': persisted_state.get('last_daily_report')
    }

def check_batch_notifications(color, gravity, temp_f, brewid, cfg):
    """
    Check and trigger batch-specific notifications:
    1. Loss of signal detection
    2. Fermentation starting detection
    3. Daily report scheduling (handled separately in periodic task)
    """
    if not brewid:
        return
    
    # Get notification settings from system config
    notif_cfg = system_cfg.get('batch_notifications', {})
    
    # Initialize state for this brewid if needed
    if brewid not in batch_notification_state:
        # Load persisted state from config file
        batch_notification_state[brewid] = load_notification_state_from_config(color, brewid, cfg)
    
    state = batch_notification_state[brewid]
    state['last_reading_time'] = datetime.utcnow()
    
    # Reset signal loss flag when we receive a reading
    if state['signal_lost']:
        state['signal_lost'] = False
        state['signal_loss_notified'] = False
        # Note: Signal loss is NOT persisted, so no save needed here
    
    # Track gravity history for fermentation start detection
    if gravity is not None:
        state['gravity_history'].append({
            'gravity': gravity,
            'timestamp': datetime.utcnow()
        })
        # Keep only recent readings (last 10)
        if len(state['gravity_history']) > 10:
            state['gravity_history'].pop(0)
    
    # Check fermentation starting condition
    if notif_cfg.get('enable_fermentation_starting', True):
        check_fermentation_starting(color, brewid, cfg, state)
    
    # Check fermentation completion condition
    if notif_cfg.get('enable_fermentation_completion', True):
        check_fermentation_completion(color, brewid, cfg, state)

def check_fermentation_starting(color, brewid, cfg, state):
    """
    Detect fermentation start: 3 consecutive readings at least 0.010 below starting gravity.
    Saves the datetime when fermentation start notification is sent.
    """
    # Check if notification already sent (datetime will be present)
    if state.get('fermentation_start_datetime'):
        return
    
    # Add debounce protection: prevent re-checking too frequently (5 second minimum)
    last_check = state.get('last_fermentation_start_check')
    if last_check:
        elapsed = (datetime.utcnow() - last_check).total_seconds()
        if elapsed < 5:
            return
    
    actual_og = cfg.get('actual_og')
    if not actual_og:
        return
    
    try:
        starting_gravity = float(actual_og)
    except (ValueError, TypeError):
        return
    
    history = state.get('gravity_history', [])
    if len(history) < 3:
        return
    
    # Check last 3 readings
    last_three = history[-3:]
    all_below_threshold = all(
        reading['gravity'] is not None and reading['gravity'] <= (starting_gravity - 0.010)
        for reading in last_three
    )
    
    if all_below_threshold:
        # Update debounce timestamp only when we detect the condition
        # This ensures we don't delay legitimate notifications due to early returns
        state['last_fermentation_start_check'] = datetime.utcnow()
        
        current_gravity = last_three[-1]['gravity']
        brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
        beer_name = cfg.get('beer_name', 'Unknown Beer')
        
        # Get current datetime for the notification
        notification_time = datetime.utcnow()
        
        # Set flag BEFORE sending to prevent race condition with duplicate notifications
        # This is critical: setting the flag FIRST ensures that even if multiple BLE packets
        # arrive within milliseconds, only the first one will proceed to send notification
        state['fermentation_start_datetime'] = notification_time.isoformat()
        state['fermentation_started'] = True
        
        subject = f"{brewery_name} - Fermentation Started"
        body = f"""Brewery Name: {brewery_name}
Tilt Color: {color}
Brew Name: {beer_name}
Date/Time: {notification_time.strftime('%Y-%m-%d %H:%M:%S')}

Fermentation has started.
Gravity at start: {starting_gravity:.3f}
Gravity now: {current_gravity:.3f}"""
        
        # Always save state to config file, regardless of notification success
        # This prevents duplicate notifications even if retry fails
        # Users can check logs/UI to see if notifications failed
        save_notification_state_to_config(color, brewid)
        
        # Queue notification with 10-second delay for deduplication
        queue_pending_notification(
            notification_type='fermentation_start',
            subject=subject,
            body=body,
            brewid=brewid,
            color=color
        )

def check_fermentation_completion(color, brewid, cfg, state):
    """
    Detect fermentation completion: gravity stable for 24 hours after fermentation started.
    Saves the datetime when fermentation completion notification is sent.
    """
    # Check if notification already sent (datetime will be present)
    if state.get('fermentation_completion_datetime'):
        return
    
    # Add debounce protection: prevent re-checking too frequently (5 second minimum)
    last_check = state.get('last_fermentation_completion_check')
    if last_check:
        elapsed = (datetime.utcnow() - last_check).total_seconds()
        if elapsed < 5:
            return
    
    # Only check for completion if fermentation has started
    if not state.get('fermentation_started'):
        return
    
    history = state.get('gravity_history', [])
    if len(history) < 2:
        return
    
    # Check if gravity has been stable for 24 hours
    # Look at last reading and compare with readings from 24 hours ago
    current_time = datetime.utcnow()
    current_gravity = history[-1]['gravity']
    
    # Find readings from approximately 24 hours ago
    stable_for_24h = True
    readings_24h_ago = []
    
    for reading in history:
        time_diff = (current_time - reading['timestamp']).total_seconds() / 3600.0
        
        # Check readings between 23-25 hours ago
        if 23 <= time_diff <= 25:
            readings_24h_ago.append(reading)
            # If gravity has changed by more than 0.002, not stable
            if abs(reading['gravity'] - current_gravity) > 0.002:
                stable_for_24h = False
                break
    
    # Need at least one reading from 24 hours ago to confirm stability
    if not readings_24h_ago or not stable_for_24h:
        return
    
    # Fermentation completion detected
    # Update debounce timestamp only when we detect the condition
    # This ensures we don't delay legitimate notifications due to early returns
    state['last_fermentation_completion_check'] = datetime.utcnow()
    
    brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
    beer_name = cfg.get('beer_name', 'Unknown Beer')
    actual_og = cfg.get('actual_og')
    
    notification_time = datetime.utcnow()
    
    # Set flag BEFORE sending to prevent race condition with duplicate notifications
    # This is critical: setting the flag FIRST ensures that even if multiple BLE packets
    # arrive within milliseconds, only the first one will proceed to send notification
    state['fermentation_completion_datetime'] = notification_time.isoformat()
    
    subject = f"{brewery_name} - Fermentation Completion"
    body = f"""Brewery Name: {brewery_name}
Tilt Color: {color}
Brew Name: {beer_name}
Date/Time: {notification_time.strftime('%Y-%m-%d %H:%M:%S')}

Fermentation completion detected.
Gravity has been stable for 24 hours: {current_gravity:.3f}"""
    
    if actual_og:
        try:
            starting_gravity = float(actual_og)
            attenuation = ((starting_gravity - current_gravity) / (starting_gravity - 1.0)) * 100
            body += f"\nStarting Gravity: {starting_gravity:.3f}"
            body += f"\nApparent Attenuation: {attenuation:.1f}%"
        except (ValueError, TypeError):
            pass
    
    # Always save state to config file, regardless of notification success
    # This prevents duplicate notifications even if retry fails
    # Users can check logs/UI to see if notifications failed
    save_notification_state_to_config(color, brewid)
    
    # Queue notification with 10-second delay for deduplication
    queue_pending_notification(
        notification_type='fermentation_completion',
        subject=subject,
        body=body,
        brewid=brewid,
        color=color
    )

def check_signal_loss():
    """
    Periodic check for loss of signal on all active tilts.
    Run this in a separate thread or periodic task.
    """
    notif_cfg = system_cfg.get('batch_notifications', {})
    if not notif_cfg.get('enable_loss_of_signal', True):
        return
    
    loss_timeout_minutes = int(notif_cfg.get('loss_of_signal_timeout_minutes', 30))
    now = datetime.utcnow()
    
    for brewid, state in batch_notification_state.items():
        if state.get('signal_loss_notified'):
            continue
        
        last_reading = state.get('last_reading_time')
        if not last_reading:
            continue
        
        elapsed_minutes = (now - last_reading).total_seconds() / 60.0
        
        if elapsed_minutes >= loss_timeout_minutes:
            # Find the tilt color and config for this brewid
            color = None
            cfg = None
            for tilt_color, tilt_data in tilt_cfg.items():
                if tilt_data.get('brewid') == brewid:
                    color = tilt_color
                    cfg = tilt_data
                    break
            
            if color and cfg:
                # Set flags BEFORE queueing to prevent race condition with duplicate notifications
                # This ensures that even if check_signal_loss is called multiple times rapidly,
                # only the first call will proceed to queue the notification
                state['signal_lost'] = True
                state['signal_loss_notified'] = True
                
                brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
                beer_name = cfg.get('beer_name', 'Unknown Beer')
                
                subject = f"{brewery_name} - Loss of Signal"
                body = f"""Brewery Name: {brewery_name}
Tilt Color: {color}
Brew Name: {beer_name}
Date/Time: {now.strftime('%Y-%m-%d %H:%M:%S')}

Loss of Signal -- Receiving no tilt readings"""
                
                # Queue notification with 10-second delay for deduplication
                queue_pending_notification(
                    notification_type='signal_loss',
                    subject=subject,
                    body=body,
                    brewid=brewid,
                    color=color
                )

def queue_notification_retry(notification_type, subject, body, brewid, color):
    """
    Queue a failed notification for retry with exponential backoff.
    
    Args:
        notification_type: Type of notification (signal_loss, fermentation_start, fermentation_completion)
        subject: Email/PUSH subject
        body: Email/PUSH body
        brewid: Brew ID for deduplication
        color: Tilt color for deduplication
    """
    # Check if this notification is already queued (prevent duplicates in retry queue)
    for item in notification_retry_queue:
        if item['notification_type'] == notification_type and item['brewid'] == brewid:
            # Already queued, don't add again
            return
    
    notification_retry_queue.append({
        'notification_type': notification_type,
        'subject': subject,
        'body': body,
        'brewid': brewid,
        'color': color,
        'retry_count': 0,
        'last_retry_time': datetime.utcnow(),
        'created_time': datetime.utcnow()
    })

def process_notification_retries():
    """
    Process the notification retry queue with exponential backoff.
    Called periodically (every 5 minutes) by the batch monitoring thread.
    """
    now = datetime.utcnow()
    items_to_remove = []
    
    for item in notification_retry_queue:
        retry_count = item['retry_count']
        last_retry_time = item['last_retry_time']
        
        # Check if we've exceeded max retries
        if retry_count >= NOTIFICATION_MAX_RETRIES:
            print(f"[LOG] Notification retry limit reached for {item['notification_type']}/{item['brewid']}, giving up")
            items_to_remove.append(item)
            continue
        
        # Calculate time since last retry
        elapsed_seconds = (now - last_retry_time).total_seconds()
        
        # Get the retry interval for this attempt
        retry_interval = NOTIFICATION_RETRY_INTERVALS[retry_count] if retry_count < len(NOTIFICATION_RETRY_INTERVALS) else NOTIFICATION_RETRY_INTERVALS[-1]
        
        # Check if it's time to retry
        if elapsed_seconds >= retry_interval:
            print(f"[LOG] Retrying notification: {item['notification_type']}/{item['brewid']} (attempt {retry_count + 2})")
            
            # Attempt to send
            success = attempt_send_notifications(item['subject'], item['body'])
            
            if success:
                print(f"[LOG] Notification retry successful for {item['notification_type']}/{item['brewid']}")
                items_to_remove.append(item)
            else:
                # Update retry count and time
                item['retry_count'] += 1
                item['last_retry_time'] = now
                print(f"[LOG] Notification retry failed for {item['notification_type']}/{item['brewid']}, will retry again")
    
    # Remove successfully sent or expired items
    for item in items_to_remove:
        notification_retry_queue.remove(item)

def queue_pending_notification(notification_type, subject, body, brewid, color):
    """
    Queue a notification in the pending queue with deduplication.
    
    This implements a 10-second delay before sending notifications to prevent duplicates.
    If the same notification is already pending, it will not be added again.
    
    Args:
        notification_type: Type of notification (signal_loss, fermentation_start, fermentation_completion)
        subject: Email/PUSH subject
        body: Email/PUSH body
        brewid: Brew ID for deduplication
        color: Tilt color for deduplication
    """
    # Check if this notification is already pending (prevent duplicates)
    for item in pending_notifications:
        if item['notification_type'] == notification_type and item['brewid'] == brewid:
            # Already pending, don't add again
            print(f"[LOG] Notification {notification_type}/{brewid} already pending, skipping duplicate")
            return
    
    # Add to pending queue
    pending_notifications.append({
        'notification_type': notification_type,
        'subject': subject,
        'body': body,
        'brewid': brewid,
        'color': color,
        'queued_time': datetime.utcnow()
    })
    print(f"[LOG] Queued {notification_type}/{brewid} notification for sending in {NOTIFICATION_PENDING_DELAY_SECONDS} seconds")

def process_pending_notifications():
    """
    Process the pending notification queue.
    
    Sends notifications that have been pending for at least NOTIFICATION_PENDING_DELAY_SECONDS.
    This provides a window for deduplication to work - if multiple identical notifications
    are triggered within the delay window, only the first one will be sent.
    
    Called periodically (every 5 minutes) by the batch monitoring thread.
    """
    now = datetime.utcnow()
    items_to_remove = []
    
    for item in pending_notifications:
        queued_time = item['queued_time']
        
        # Calculate time since queued
        elapsed_seconds = (now - queued_time).total_seconds()
        
        # Check if it's time to send (after the delay period)
        if elapsed_seconds >= NOTIFICATION_PENDING_DELAY_SECONDS:
            print(f"[LOG] Sending pending notification: {item['notification_type']}/{item['brewid']}")
            
            # Attempt to send
            success = attempt_send_notifications(item['subject'], item['body'])
            
            if success:
                print(f"[LOG] Pending notification sent successfully for {item['notification_type']}/{item['brewid']}")
                items_to_remove.append(item)
            else:
                # Failed to send - queue for retry
                print(f"[LOG] Pending notification failed for {item['notification_type']}/{item['brewid']}, queuing for retry")
                queue_notification_retry(
                    notification_type=item['notification_type'],
                    subject=item['subject'],
                    body=item['body'],
                    brewid=item['brewid'],
                    color=item['color']
                )
                items_to_remove.append(item)
    
    # Remove sent or failed items
    for item in items_to_remove:
        pending_notifications.remove(item)

def send_daily_report():
    """
    Send daily progress report for all active tilts.
    Should be scheduled to run at user-specified time.
    """
    notif_cfg = system_cfg.get('batch_notifications', {})
    if not notif_cfg.get('enable_daily_report', True):
        return
    
    brewery_name = system_cfg.get('brewery_name', 'Unknown Brewery')
    
    for color, cfg in tilt_cfg.items():
        brewid = cfg.get('brewid')
        if not brewid:
            continue
        
        # Check if we have recent data for this tilt
        if color not in live_tilts:
            continue
        
        state = batch_notification_state.get(brewid, {})
        
        # Check if we already sent today's report (within last 23 hours to allow for timing variance)
        last_report = state.get('last_daily_report')
        if last_report:
            try:
                last_report_dt = datetime.fromisoformat(last_report)
                # Use DAILY_REPORT_COOLDOWN_HOURS to ensure daily (once per ~24h) but allow for timing variance
                if (datetime.utcnow() - last_report_dt).total_seconds() < DAILY_REPORT_COOLDOWN_HOURS * 3600:
                    continue
            except Exception:
                pass
        
        beer_name = cfg.get('beer_name', 'Unknown Beer')
        actual_og = cfg.get('actual_og')
        
        if not actual_og:
            continue
        
        try:
            starting_gravity = float(actual_og)
        except (ValueError, TypeError):
            continue
        
        current_data = live_tilts.get(color, {})
        current_gravity = current_data.get('gravity')
        
        if current_gravity is None:
            continue
        
        net_change = starting_gravity - current_gravity
        
        # Calculate change since yesterday (24 hours ago)
        change_since_yesterday = 0.0
        history = state.get('gravity_history', [])
        if history:
            # Find reading closest to 24 hours ago
            target_time = datetime.utcnow() - timedelta(hours=24)
            closest_reading = None
            min_diff = float('inf')
            
            for reading in history:
                time_diff = abs((reading['timestamp'] - target_time).total_seconds())
                if time_diff < min_diff:
                    min_diff = time_diff
                    closest_reading = reading
            
            if closest_reading and closest_reading['gravity'] is not None:
                change_since_yesterday = closest_reading['gravity'] - current_gravity
        
        subject = f"{brewery_name} - Daily Report"
        body = f"""Brewery Name: {brewery_name}
Tilt Color: {color}
Brew Name: {beer_name}
Date/Time: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}

Starting Gravity: {starting_gravity:.3f}
Last Gravity: {current_gravity:.3f}
Net Change: {net_change:.3f}
Change since yesterday: {change_since_yesterday:.3f}"""
        
        if attempt_send_notifications(subject, body):
            state['last_daily_report'] = datetime.utcnow().isoformat()
            save_notification_state_to_config(color, brewid)

# --- Kasa command dedupe & rate limit -------------------------------------
_last_kasa_command = {}
_KASA_RATE_LIMIT_SECONDS = int(system_cfg.get("kasa_rate_limit_seconds", 10) or 10)

def _should_send_kasa_command(url, action):
    if not url:
        return False
    if not kasa_worker:
        return False
    if url == temp_cfg.get("heating_plug") and temp_cfg.get("heater_pending"):
        return False
    if url == temp_cfg.get("cooling_plug") and temp_cfg.get("cooler_pending"):
        return False
    if url == temp_cfg.get("heating_plug"):
        if temp_cfg.get("heater_on") and action == "on":
            return False
        if (not temp_cfg.get("heater_on")) and action == "off":
            return False
    if url == temp_cfg.get("cooling_plug"):
        if temp_cfg.get("cooler_on") and action == "on":
            return False
        if (not temp_cfg.get("cooler_on")) and action == "off":
            return False
    last = _last_kasa_command.get(url)
    if last and last.get("action") == action:
        if (time.time() - last.get("ts", 0.0)) < _KASA_RATE_LIMIT_SECONDS:
            return False
    return True

def _record_kasa_command(url, action):
    _last_kasa_command[url] = {"action": action, "ts": time.time()}

# --- Control functions -----------------------------------------------------
def control_heating(state):
    enabled = temp_cfg.get("enable_heating")
    url = temp_cfg.get("heating_plug", "")
    if not enabled or not url:
        temp_cfg["heater_pending"] = False
        temp_cfg["heater_on"] = False
        # Clear heating errors when heating is disabled
        temp_cfg["heating_error"] = False
        temp_cfg["heating_error_msg"] = ""
        temp_cfg["heating_error_notified"] = False
        return
    if not _should_send_kasa_command(url, state):
        print(f"[TEMP_CONTROL] Skipping heating {state} command (redundant or rate-limited)")
        return
    print(f"[TEMP_CONTROL] Sending heating {state.upper()} command to {url}")
    kasa_queue.put({'mode': 'heating', 'url': url, 'action': state})
    _record_kasa_command(url, state)
    temp_cfg["heater_pending"] = True

def control_cooling(state):
    enabled = temp_cfg.get("enable_cooling")
    url = temp_cfg.get("cooling_plug", "")
    if not enabled or not url:
        temp_cfg["cooler_pending"] = False
        temp_cfg["cooler_on"] = False
        # Clear cooling errors when cooling is disabled
        temp_cfg["cooling_error"] = False
        temp_cfg["cooling_error_msg"] = ""
        temp_cfg["cooling_error_notified"] = False
        return
    if not _should_send_kasa_command(url, state):
        print(f"[TEMP_CONTROL] Skipping cooling {state} command (redundant or rate-limited)")
        return
    print(f"[TEMP_CONTROL] Sending cooling {state.upper()} command to {url}")
    kasa_queue.put({'mode': 'cooling', 'url': url, 'action': state})
    _record_kasa_command(url, state)
    temp_cfg["cooler_pending"] = True

# --- Temperature control logic (normalized + limited logging) -------------
def temperature_control_logic():
    """
    Main control loop logic.

    Important behavior change:
    - If temp_control_enabled is False, the function will NOT modify or clear the stored
      temp_cfg fields (heater_on, cooler_on, pending flags, limits, plugs, etc.).
      It only sets a 'Disabled' status and returns. This preserves configuration so that
      when the controller is turned back on the previous settings are used as the
      starting point.
    - All control actions (control_heating/control_cooling) are skipped while disabled.
    """
    # If the overall temp control subsystem is disabled, do not perform any actions.
    # Preserve the saved configuration and active-state flags — don't clear them.
    if not temp_cfg.get("temp_control_enabled", True):
        temp_cfg['status'] = "Disabled"
        # Do NOT change heater_on/cooler_on/heater_pending/cooler_pending or limits here.
        # Returning early prevents any control commands from being issued.
        return

    enable_heat = bool(temp_cfg.get("enable_heating"))
    enable_cool = bool(temp_cfg.get("enable_cooling"))
    if enable_heat and enable_cool:
        temp_cfg['mode'] = "Heating and Cooling Selected"
    elif enable_heat:
        temp_cfg['mode'] = "Heating Selected"
    elif enable_cool:
        temp_cfg['mode'] = "Cooling Selected"
    else:
        temp_cfg['mode'] = "Off"

    temp = temp_cfg.get("current_temp")
    if temp is None:
        temp_from_tilt = get_current_temp_for_control_tilt()
        if temp_from_tilt is not None:
            try:
                temp = float(temp_from_tilt)
                temp_cfg['current_temp'] = round(temp, 1)
            except Exception:
                temp = None

    low = temp_cfg.get("low_limit")
    high = temp_cfg.get("high_limit")
    
    # Check if temp control monitoring is active
    is_monitoring_active = bool(temp_cfg.get("temp_control_active"))

    if not temp_cfg.get("control_initialized"):
        if enable_heat or enable_cool:
            append_control_log("temp_control_mode", {
                "low_limit": low,
                "current_temp": temp,
                "high_limit": high,
                "tilt_color": temp_cfg.get("tilt_color", "")
            })
        temp_cfg["control_initialized"] = True
        temp_cfg["last_logged_low_limit"] = low
        temp_cfg["last_logged_high_limit"] = high
        temp_cfg["last_logged_enable_heating"] = enable_heat
        temp_cfg["last_logged_enable_cooling"] = enable_cool

    if (temp_cfg.get("last_logged_low_limit") != low or
        temp_cfg.get("last_logged_high_limit") != high or
        temp_cfg.get("last_logged_enable_heating") != enable_heat or
        temp_cfg.get("last_logged_enable_cooling") != enable_cool):
        if enable_heat or enable_cool:
            append_control_log("temp_control_mode_changed", {
                "low_limit": low,
                "current_temp": temp,
                "high_limit": high,
                "tilt_color": temp_cfg.get("tilt_color", "")
            })
        temp_cfg["last_logged_low_limit"] = low
        temp_cfg["last_logged_high_limit"] = high
        temp_cfg["last_logged_enable_heating"] = enable_heat
        temp_cfg["last_logged_enable_cooling"] = enable_cool

    # SAFETY: Check if control Tilt is active (within timeout)
    # If any Tilt being used for temp control is inactive, turn off all plugs immediately
    # This includes both explicitly assigned Tilts and fallback Tilts
    if not is_control_tilt_active():
        control_heating("off")
        control_cooling("off")
        
        # Get the actual Tilt color being used (may be explicitly assigned or fallback)
        actual_tilt_color = get_control_tilt_color()
        assigned_tilt_color = temp_cfg.get("tilt_color", "")
        
        # Set status message indicating which Tilt triggered shutdown
        if assigned_tilt_color:
            temp_cfg["status"] = f"Control Tilt Inactive - Safety Shutdown ({assigned_tilt_color})"
        elif actual_tilt_color:
            temp_cfg["status"] = f"Control Tilt Inactive - Safety Shutdown (using {actual_tilt_color} as fallback)"
        else:
            temp_cfg["status"] = "Control Tilt Inactive - Safety Shutdown"
        
        # Log this safety event and send notification
        if not temp_cfg.get("safety_shutdown_logged"):
            # Use the actual Tilt color for logging
            tilt_color = actual_tilt_color or assigned_tilt_color or "Unknown"
            append_control_log("temp_control_safety_shutdown", {
                "tilt_color": tilt_color,
                "assigned_tilt": assigned_tilt_color,
                "actual_tilt": actual_tilt_color or "None",
                "reason": "Control Tilt inactive beyond timeout",
                "low_limit": low,
                "high_limit": high
            })
            temp_cfg["safety_shutdown_logged"] = True
            
            # Send safety shutdown notification
            send_safety_shutdown_notification(tilt_color, low, high)
        return
    else:
        # Reset the safety shutdown flag when Tilt becomes active again
        if temp_cfg.get("safety_shutdown_logged"):
            temp_cfg["safety_shutdown_logged"] = False

    if temp is None:
        control_heating("off")
        control_cooling("off")
        temp_cfg["status"] = "Device Offline"
        return

    current_action = None
    
    # Calculate midpoint for hysteresis control
    midpoint = None
    if isinstance(low, (int, float)) and isinstance(high, (int, float)):
        midpoint = (low + high) / 2.0
        
        # Safety check: Ensure low_limit is less than high_limit
        if low >= high:
            temp_cfg["status"] = "Configuration Error: Low limit must be less than high limit"
            control_heating("off")
            control_cooling("off")
            return

    # Heating control with hysteresis:
    # - Turn ON when temp <= low_limit
    # - Turn OFF when temp >= midpoint
    if enable_heat:
        if temp <= low:
            # Temperature at or below low limit - turn heating ON
            control_heating("on")
            current_action = "Heating"
            # Log with trigger when temp goes below low limit
            if temp_cfg.get("below_limit_trigger_armed") and is_monitoring_active:
                append_control_log("temp_below_low_limit", {"low_limit": low, "current_temp": temp, "high_limit": high, "tilt_color": temp_cfg.get("tilt_color", "")})
                temp_cfg["below_limit_logged"] = True
                # Send notification if enabled
                send_temp_control_notification("temp_below_low_limit", temp, low, high, temp_cfg.get("tilt_color", ""))
                temp_cfg["below_limit_trigger_armed"] = False
                temp_cfg["above_limit_trigger_armed"] = False  # Ensure above is disarmed
        elif midpoint is not None and temp >= midpoint:
            # Temperature at or above midpoint - turn heating OFF
            control_heating("off")
        # else: temperature is between low and midpoint - maintain current state
        # (don't change heating state, let it continue)
    else:
        control_heating("off")

    # Cooling control with hysteresis:
    # - Turn ON when temp >= high_limit
    # - Turn OFF when temp <= midpoint
    if enable_cool:
        if temp >= high:
            # Temperature at or above high limit - turn cooling ON
            control_cooling("on")
            current_action = "Cooling"
            # Log with trigger when temp goes above high limit
            if temp_cfg.get("above_limit_trigger_armed") and is_monitoring_active:
                append_control_log("temp_above_high_limit", {"low_limit": low, "current_temp": temp, "high_limit": high, "tilt_color": temp_cfg.get("tilt_color", "")})
                temp_cfg["above_limit_logged"] = True
                # Send notification if enabled
                send_temp_control_notification("temp_above_high_limit", temp, low, high, temp_cfg.get("tilt_color", ""))
                temp_cfg["above_limit_trigger_armed"] = False
                temp_cfg["below_limit_trigger_armed"] = False  # Ensure below is disarmed
        elif midpoint is not None and temp <= midpoint:
            # Temperature at or below midpoint - turn cooling OFF
            control_cooling("off")
        # else: temperature is between midpoint and high - maintain current state
        # (don't change cooling state, let it continue)
    else:
        control_cooling("off")
    
    # Update current_action based on actual plug state
    if temp_cfg.get("heater_on"):
        current_action = "Heating"
    elif temp_cfg.get("cooler_on"):
        current_action = "Cooling"
    
    # Safety check: Both heating and cooling should never be ON simultaneously
    if enable_heat and enable_cool:
        if temp_cfg.get("heater_on") and temp_cfg.get("cooler_on"):
            # This should never happen, but if it does, turn both off for safety
            control_heating("off")
            control_cooling("off")
            temp_cfg["status"] = "Safety Error: Both heating and cooling were ON"
            append_control_log("temp_control_safety_shutdown", {
                "tilt_color": temp_cfg.get("tilt_color", ""),
                "reason": "Both heating and cooling active simultaneously",
                "low_limit": low,
                "high_limit": high,
                "current_temp": temp
            })
            return

    try:
        if isinstance(low, (int, float)) and isinstance(high, (int, float)) and (low <= temp <= high):
            # Temperature is in range
            # Log with trigger when entering range
            if temp_cfg.get("in_range_trigger_armed") and is_monitoring_active:
                append_control_log("temp_in_range", {"low_limit": low, "current_temp": temp, "high_limit": high, "tilt_color": temp_cfg.get("tilt_color", "")})
                temp_cfg["in_range_trigger_armed"] = False
            # Re-arm the out-of-range triggers when in range
            temp_cfg["above_limit_trigger_armed"] = True
            temp_cfg["below_limit_trigger_armed"] = True
            temp_cfg["status"] = "In Range"
            return
        else:
            # Temperature is out of range - re-arm in_range trigger
            temp_cfg["in_range_trigger_armed"] = True
    except Exception:
        pass

    if current_action == "Heating":
        temp_cfg["status"] = "Heating"
    elif current_action == "Cooling":
        temp_cfg["status"] = "Cooling"
    else:
        temp_cfg["status"] = "Idle"

# --- kasa result listener (log confirmed ON/OFF events) --------------------
def kasa_result_listener():
    while True:
        try:
            result = kasa_result_queue.get(timeout=5)
            mode = result.get('mode')
            action = result.get('action')
            success = result.get('success', False)
            url = result.get('url', '')
            error = result.get('error', '')
            
            print(f"[KASA_RESULT] Received result: mode={mode}, action={action}, success={success}, url={url}, error={error}")
            
            if mode == 'heating':
                temp_cfg["heater_pending"] = False
                if success:
                    temp_cfg["heater_on"] = (action == 'on')
                    temp_cfg["heating_error"] = False
                    temp_cfg["heating_error_msg"] = ""
                    # Reset the notified flag when plug starts working again
                    temp_cfg["heating_error_notified"] = False
                    event = "heating_on" if action == 'on' else "heating_off"
                    print(f"[KASA_RESULT] ✓ Heating plug {action.upper()} confirmed - updating heater_on={action == 'on'}")
                    append_control_log(event, {"low_limit": temp_cfg.get("low_limit"), "current_temp": temp_cfg.get("current_temp"), "high_limit": temp_cfg.get("high_limit"), "tilt_color": temp_cfg.get("tilt_color", "")})
                    # Send notification if enabled (user can choose to enable/disable)
                    send_temp_control_notification(event, temp_cfg.get("current_temp", 0), temp_cfg.get("low_limit", 0), temp_cfg.get("high_limit", 0), temp_cfg.get("tilt_color", ""))
                else:
                    # When plug command fails, ensure heater_on is False for accurate UI state
                    temp_cfg["heater_on"] = False
                    temp_cfg["heating_error"] = True
                    temp_cfg["heating_error_msg"] = error or ''
                    print(f"[KASA_RESULT] ✗ Heating plug {action.upper()} FAILED - error: {error}")
                    # Log error to kasa_errors.log
                    log_error(f"HEATING plug {action.upper()} failed at {url}: {error}")
                    # Send notification for Kasa connection failure if enabled (only once per failure)
                    send_kasa_error_notification('heating', url, error)
            elif mode == 'cooling':
                temp_cfg["cooler_pending"] = False
                if success:
                    temp_cfg["cooler_on"] = (action == 'on')
                    temp_cfg["cooling_error"] = False
                    temp_cfg["cooling_error_msg"] = ""
                    # Reset the notified flag when plug starts working again
                    temp_cfg["cooling_error_notified"] = False
                    event = "cooling_on" if action == 'on' else "cooling_off"
                    print(f"[KASA_RESULT] ✓ Cooling plug {action.upper()} confirmed - updating cooler_on={action == 'on'}")
                    append_control_log(event, {"low_limit": temp_cfg.get("low_limit"), "current_temp": temp_cfg.get("current_temp"), "high_limit": temp_cfg.get("high_limit"), "tilt_color": temp_cfg.get("tilt_color", "")})
                    # Send notification if enabled (user can choose to enable/disable)
                    send_temp_control_notification(event, temp_cfg.get("current_temp", 0), temp_cfg.get("low_limit", 0), temp_cfg.get("high_limit", 0), temp_cfg.get("tilt_color", ""))
                else:
                    # When plug command fails, ensure cooler_on is False for accurate UI state
                    temp_cfg["cooler_on"] = False
                    temp_cfg["cooling_error"] = True
                    temp_cfg["cooling_error_msg"] = error or ''
                    print(f"[KASA_RESULT] ✗ Cooling plug {action.upper()} FAILED - error: {error}")
                    # Log error to kasa_errors.log
                    log_error(f"COOLING plug {action.upper()} failed at {url}: {error}")
                    # Send notification for Kasa connection failure if enabled (only once per failure)
                    send_kasa_error_notification('cooling', url, error)
        except queue.Empty:
            # Timeout waiting for result - this is normal, just continue
            continue
        except Exception as e:
            # Log unexpected exceptions to help with debugging
            print(f"[LOG] Exception in kasa_result_listener: {e}")
            continue

threading.Thread(target=kasa_result_listener, daemon=True).start()

# --- Startup plug state synchronization -------------------------------------
def sync_plug_states_at_startup():
    """
    Synchronize stored plug states with actual plug states at startup.
    This prevents displaying stale state from the last shutdown.
    
    This function runs with a timeout to prevent blocking Flask startup.
    If queries take too long, it keeps the current state and lets the
    control loop handle synchronization.
    """
    if kasa_query_state is None:
        print("[LOG] kasa_query_state not available, skipping startup sync")
        return
    
    heating_url = temp_cfg.get("heating_plug", "")
    cooling_url = temp_cfg.get("cooling_plug", "")
    enable_heating = temp_cfg.get("enable_heating", False)
    enable_cooling = temp_cfg.get("enable_cooling", False)
    
    print("[LOG] Syncing plug states at startup...")
    
    # Helper function to query plug state with timeout
    async def query_plug_with_timeout(url, plug_name):
        """Query a plug's state with timeout. Returns (is_on, error) or raises TimeoutError."""
        # Timeout set to 7 seconds to allow internal kasa_query_state timeout (6s) to complete
        return await asyncio.wait_for(kasa_query_state(url), timeout=7.0)
    
    # Query heating plug state with timeout
    if enable_heating and heating_url:
        try:
            is_on, error = asyncio.run(query_plug_with_timeout(heating_url, "heating"))
            if error is None:
                temp_cfg["heater_on"] = is_on
                print(f"[LOG] Heating plug state synced: {'ON' if is_on else 'OFF'}")
            else:
                # If we can't query the state, keep current value and let control loop handle it
                # Don't override to False as this causes UI flicker when network is slow
                print(f"[LOG] Failed to query heating plug state: {error}, keeping current state")
        except asyncio.TimeoutError:
            # If query times out, keep current value and let control loop handle it
            print(f"[LOG] Heating plug query timed out, keeping current state")
        except Exception as e:
            # If query fails, keep current value and let control loop handle it
            print(f"[LOG] Error querying heating plug: {e}, keeping current state")
    else:
        # Only set to False if heating is not enabled
        temp_cfg["heater_on"] = False
    
    # Query cooling plug state with timeout
    if enable_cooling and cooling_url:
        try:
            is_on, error = asyncio.run(query_plug_with_timeout(cooling_url, "cooling"))
            if error is None:
                temp_cfg["cooler_on"] = is_on
                print(f"[LOG] Cooling plug state synced: {'ON' if is_on else 'OFF'}")
            else:
                # If we can't query the state, keep current value and let control loop handle it
                # Don't override to False as this causes UI flicker when network is slow
                print(f"[LOG] Failed to query cooling plug state: {error}, keeping current state")
        except asyncio.TimeoutError:
            # If query times out, keep current value and let control loop handle it
            print(f"[LOG] Cooling plug query timed out, keeping current state")
        except Exception as e:
            # If query fails, keep current value and let control loop handle it
            print(f"[LOG] Error querying cooling plug: {e}, keeping current state")
    else:
        # Only set to False if cooling is not enabled
        temp_cfg["cooler_on"] = False
    
    print("[LOG] Plug state synchronization complete")
    
    # Log the startup sync to the control log
    try:
        append_control_log("startup_plug_sync", {
            "heater_on": temp_cfg.get("heater_on"),
            "cooler_on": temp_cfg.get("cooler_on"),
            "enable_heating": enable_heating,
            "enable_cooling": enable_cooling,
            "heating_url": heating_url if enable_heating else "",
            "cooling_url": cooling_url if enable_cooling else ""
        })
    except Exception as e:
        print(f"[LOG] Failed to log startup sync: {e}")

# Start sync in background thread to prevent blocking Flask startup
# This allows the web server to start immediately even if plug queries are slow
def _background_startup_sync():
    """Run startup sync in background to avoid blocking Flask"""
    try:
        # Small delay (0.5s) to let kasa_result_listener thread initialize
        # This is a minimal delay chosen to ensure basic thread startup without
        # significantly delaying the sync. The sync will handle failures gracefully.
        time.sleep(0.5)
        sync_plug_states_at_startup()
    except Exception as e:
        print(f"[LOG] Exception in background startup sync: {e}")

threading.Thread(target=_background_startup_sync, daemon=True).start()

# --- Offsite push helpers (kept, forwarding enabled) -----------------------
def get_predefined_field_maps():
    """
    Returns a dictionary of predefined field map templates that users can select from.
    These are common field mappings for popular services.
    """
    return {
        "default": {
            "name": "Default",
            "description": "Standard field names",
            "map": {
                "timestamp": "timestamp",
                "tilt_color": "tilt_color",
                "gravity": "gravity",
                "temp_f": "temp",
                "brew_id": "brewid",
                "device": "device"
            }
        },
        "brewersfriend": {
            "name": "Brewers Friend",
            "description": "Optimized for Brewers Friend API",
            "map": {
                "timestamp": "timestamp",
                "tilt_color": "name",
                "gravity": "gravity",
                "temp_f": "temp",
                "brew_id": "beer",
                "device": "device"
            }
        },
        "custom": {
            "name": "Custom",
            "description": "User-defined field mapping",
            "map": {
                "timestamp": "timestamp",
                "tilt_color": "tilt_color",
                "gravity": "gravity",
                "temp_f": "temp",
                "brew_id": "brewid",
                "device": "device"
            }
        }
    }

def build_offsite_payload(field_map=None):
    default_map = {
        'timestamp': 'timestamp',
        'tilt_color': 'tilt_color',
        'gravity': 'gravity',
        'temp_f': 'temp',
        'brew_id': 'brewid',
        'device': 'device'
    }
    if not field_map:
        field_map = default_map
    payload = {
        'timestamp': datetime.utcnow().isoformat(),
        'temp_control': {
            'current_temp': temp_cfg.get("current_temp"),
            'low_limit': temp_cfg.get("low_limit"),
            'high_limit': temp_cfg.get("high_limit"),
            'status': temp_cfg.get("status"),
        },
        'tilts': []
    }
    for color, info in live_tilts.items():
        entry = {
            field_map.get('tilt_color', 'tilt_color'): color,
            field_map.get('gravity', 'gravity'): info.get('gravity'),
            field_map.get('temp_f', 'temp'): info.get('temp_f'),
            field_map.get('brew_id', 'brewid'): info.get('brewid'),
            field_map.get('device', 'device'): color
        }
        payload['tilts'].append(entry)
    return payload

def push_offsite_snapshot():
    return

# --- Periodic temp control thread -----------------------------------------
def periodic_temp_control():
    while True:
        try:
            file_cfg = load_json(TEMP_CFG_FILE, {})
            if 'current_temp' in file_cfg and file_cfg['current_temp'] is None and temp_cfg.get('current_temp') is not None:
                file_cfg.pop('current_temp')
            temp_cfg.update(file_cfg)
            temperature_control_logic()
        except Exception as e:
            append_control_log("temp_control_mode_changed", {"low_limit": temp_cfg.get("low_limit"), "current_temp": temp_cfg.get("current_temp"), "high_limit": temp_cfg.get("high_limit"), "tilt_color": temp_cfg.get("tilt_color", "")})
            print("[LOG] Exception in periodic_temp_control:", e)

        try:
            interval_minutes = int(system_cfg.get("update_interval", 1))
        except Exception:
            interval_minutes = 1
        interval_seconds = max(1, interval_minutes * 60)
        time.sleep(interval_seconds)

threading.Thread(target=periodic_temp_control, daemon=True).start()

# --- Periodic batch monitoring thread -------------------------------------
def periodic_batch_monitoring():
    """Monitor for signal loss, schedule daily reports, and process notification retries."""
    last_daily_check = None
    
    while True:
        try:
            # Check for signal loss every 5 minutes
            check_signal_loss()
            
            # Process pending notifications (10-second delay for deduplication)
            process_pending_notifications()
            
            # Process notification retry queue with exponential backoff
            process_notification_retries()
            
            # Check if it's time for daily reports
            notif_cfg = system_cfg.get('batch_notifications', {})
            daily_report_time = notif_cfg.get('daily_report_time', '09:00')  # Default 9 AM
            
            now = datetime.utcnow()
            current_time_str = now.strftime('%H:%M')
            
            # Check if we should send daily report (within 5 minute window)
            if daily_report_time:
                try:
                    report_hour, report_min = map(int, daily_report_time.split(':'))
                    current_hour = now.hour
                    current_min = now.minute
                    
                    # Check if current time is within DAILY_REPORT_WINDOW_MINUTES of report time
                    time_match = (current_hour == report_hour and 
                                 abs(current_min - report_min) < DAILY_REPORT_WINDOW_MINUTES)
                    
                    # Only send once per day
                    if time_match:
                        if not last_daily_check or (now - last_daily_check).total_seconds() > 3600:
                            send_daily_report()
                            last_daily_check = now
                except Exception as e:
                    print(f"[LOG] Error checking daily report time: {e}")
        
        except Exception as e:
            print(f"[LOG] Exception in periodic_batch_monitoring: {e}")
        
        # Sleep for BATCH_MONITORING_INTERVAL_SECONDS
        time.sleep(BATCH_MONITORING_INTERVAL_SECONDS)

threading.Thread(target=periodic_batch_monitoring, daemon=True).start()

# --- BLE scanner thread ---------------------------------------------------
def ble_loop():
    async def run_scanner():
        if BleakScanner is None:
            print("[LOG] BleakScanner not available; BLE scanning disabled")
            return
        scanner = BleakScanner(detection_callback)
        await scanner.start()
        while True:
            await asyncio.sleep(5)
            try:
                temp = get_current_temp_for_control_tilt()
                if temp is not None:
                    temp_cfg['current_temp'] = round(float(temp), 1)
            except Exception as e:
                print(f"[LOG] Error in ble_loop run_scanner: {e}")
    try:
        asyncio.run(run_scanner())
    except Exception as e:
        print(f"[LOG] BLE loop failed to start: {e}")

threading.Thread(target=ble_loop, daemon=True).start()

# --- Flask routes ---------------------------------------------------------
@app.route('/')
def dashboard():
    # Only show active tilts on the main display
    active_tilts = get_active_tilts()
    return render_template('maindisplay.html',
        system_settings=system_cfg,
        tilt_cfg=tilt_cfg,
        COLOR_MAP=COLOR_MAP,
        tilts=active_tilts,
        tilt_status=tilt_status,
        temp_control=temp_cfg,
        live_tilts=active_tilts
    )

@app.route('/startup')
def startup():
    """Display startup splash screen"""
    return render_template('startup.html')

@app.route('/system_config')
def system_config():
    # Get the tab parameter from query string and validate it
    active_tab = request.args.get('tab', 'main-settings')
    if active_tab not in VALID_SYSTEM_CONFIG_TABS:
        active_tab = 'main-settings'
    
    # Migrate old format to new format if needed
    external_urls = system_cfg.get("external_urls", [])
    
    # If no external_urls but old format exists, migrate
    if not external_urls:
        for i in range(3):
            name = system_cfg.get(f"external_name_{i}", "").strip()
            url = system_cfg.get(f"external_url_{i}", "").strip()
            if url:
                url_config = {
                    "name": name or f"Service {i + 1}",
                    "url": url,
                    "method": system_cfg.get("external_method", "POST"),
                    "content_type": system_cfg.get("external_content_type", "form"),
                    "timeout_seconds": int(system_cfg.get("external_timeout_seconds", 8)),
                    "field_map_id": "default"
                }
                # If there's a custom field map, use it
                if system_cfg.get("external_field_map"):
                    url_config["field_map_id"] = "custom"
                    url_config["custom_field_map"] = system_cfg.get("external_field_map")
                external_urls.append(url_config)
    
    # Ensure we have exactly 3 slots (fill with empty ones if needed)
    while len(external_urls) < 3:
        external_urls.append({
            "name": "",
            "url": "",
            "method": "POST",
            "content_type": "form",
            "timeout_seconds": 8,
            "field_map_id": "default"
        })
    
    return render_template('system_config.html', 
                         system_settings=system_cfg,
                         external_urls=external_urls,
                         predefined_field_maps=get_predefined_field_maps(),
                         active_tab=active_tab)

@app.route('/update_system_config', methods=['POST'])
def update_system_config():
    data = request.form
    old_warn = system_cfg.get('warning_mode', 'NONE')
    
    # Capture the active tab to return to it after saving (validate against whitelist)
    active_tab = data.get('active_tab', 'main-settings')
    if active_tab not in VALID_SYSTEM_CONFIG_TABS:
        active_tab = 'main-settings'
    
    # Handle password field - only update if provided
    sending_email_password = data.get("sending_email_password", "")
    if sending_email_password:
        # Store as smtp_password for SMTP authentication
        system_cfg["smtp_password"] = sending_email_password
    
    # Handle Pushover API Token - only update if provided
    pushover_api_token = data.get("pushover_api_token", "")
    if pushover_api_token:
        system_cfg["pushover_api_token"] = pushover_api_token
    
    # Handle ntfy Auth Token - only update if provided
    ntfy_auth_token = data.get("ntfy_auth_token", "")
    if ntfy_auth_token:
        system_cfg["ntfy_auth_token"] = ntfy_auth_token
    
    # Handle external URLs - support new per-URL configuration format
    external_urls = []
    for i in range(3):
        name = data.get(f"external_name_{i}", "").strip()
        url = data.get(f"external_url_{i}", "").strip()
        
        # Always create entry (even if URL is empty) to preserve settings
        url_config = {
            "name": name or f"Service {i + 1}",
            "url": url,
            "method": data.get(f"external_method_{i}", "POST"),
            "content_type": data.get(f"external_content_type_{i}", "form"),
            "timeout_seconds": int(data.get(f"external_timeout_seconds_{i}", 8)),
            "field_map_id": data.get(f"external_field_map_id_{i}", "default")
        }
        
        # If custom field map is selected, store the custom JSON
        if url_config["field_map_id"] == "custom":
            custom_map = data.get(f"external_custom_field_map_{i}", "").strip()
            if custom_map:
                url_config["custom_field_map"] = custom_map
        
        external_urls.append(url_config)
    
    system_cfg.update({
        "brewery_name": data.get("brewery_name", ""),
        "brewer_name": data.get("brewer_name", ""),
        "street": data.get("street", ""),
        "city": data.get("city", ""),
        "state": data.get("state", ""),
        "email": data.get("email", ""),
        "mobile": data.get("mobile", ""),
        "timezone": data.get("timezone", ""),
        "timestamp_format": data.get("timestamp_format", ""),
        "display_mode": data.get("display_mode", "4"),
        "update_interval": data.get("update_interval", "1"),
        "temp_logging_interval": data.get("temp_logging_interval", system_cfg.get('temp_logging_interval', 10)),
        "external_refresh_rate": data.get("external_refresh_rate", system_cfg.get("external_refresh_rate", "15")),
        "external_urls": external_urls,  # New format
        "warning_mode": data.get("warning_mode", "NONE"),
        "sending_email": data.get("sending_email", system_cfg.get('sending_email','')),
        "smtp_host": data.get("smtp_host", system_cfg.get('smtp_host', 'smtp.gmail.com')),
        "smtp_port": int(data.get("smtp_port", system_cfg.get('smtp_port', 587))),
        "smtp_starttls": 'smtp_starttls' in data,
        "kasa_rate_limit_seconds": data.get("kasa_rate_limit_seconds", system_cfg.get('kasa_rate_limit_seconds', 10)),
        "tilt_logging_interval_minutes": int(data.get("tilt_logging_interval_minutes", system_cfg.get("tilt_logging_interval_minutes", 15))),
        "chart_temp_margin": float(data.get("chart_temp_margin", system_cfg.get("chart_temp_margin", 1.0))),
        "chart_gravity_margin": float(data.get("chart_gravity_margin", system_cfg.get("chart_gravity_margin", 0.005))),
        # Push notification provider settings
        "push_provider": data.get("push_provider", system_cfg.get("push_provider", "pushover")),
        "pushover_user_key": data.get("pushover_user_key", system_cfg.get("pushover_user_key", "")),
        "pushover_device": data.get("pushover_device", system_cfg.get("pushover_device", "")),
        "ntfy_server": data.get("ntfy_server", system_cfg.get("ntfy_server", "https://ntfy.sh")),
        "ntfy_topic": data.get("ntfy_topic", system_cfg.get("ntfy_topic", "")),
    })
    
    # Preserve old format fields for backward compatibility (only if external_urls is empty)
    if not external_urls:
        system_cfg.update({
            "external_name_0": data.get("external_name_0", system_cfg.get('external_name_0','')),
            "external_url_0": data.get("external_url_0", system_cfg.get('external_url_0','')),
            "external_name_1": data.get("external_name_1", system_cfg.get('external_name_1','')),
            "external_url_1": data.get("external_url_1", system_cfg.get('external_url_1','')),
            "external_name_2": data.get("external_name_2", system_cfg.get('external_name_2','')),
            "external_url_2": data.get("external_url_2", system_cfg.get('external_url_2','')),
            "external_method": data.get("external_method", system_cfg.get('external_method','POST')),
            "external_content_type": data.get("external_content_type", system_cfg.get('external_content_type','form')),
            "external_timeout_seconds": data.get("external_timeout_seconds", system_cfg.get('external_timeout_seconds',8)),
            "external_field_map": data.get("external_field_map", system_cfg.get('external_field_map','')),
        })
    
    # Update temperature control notifications settings
    temp_control_notif = {
        'enable_temp_below_low_limit': 'enable_temp_below_low_limit' in data,
        'enable_temp_above_high_limit': 'enable_temp_above_high_limit' in data,
        'enable_heating_on': 'enable_heating_on' in data,
        'enable_heating_off': 'enable_heating_off' in data,
        'enable_cooling_on': 'enable_cooling_on' in data,
        'enable_cooling_off': 'enable_cooling_off' in data,
        'enable_kasa_error': 'enable_kasa_error' in data,
    }
    system_cfg['temp_control_notifications'] = temp_control_notif
    
    # Update batch notifications settings
    batch_notif = {
        'enable_loss_of_signal': 'enable_loss_of_signal' in data,
        'loss_of_signal_timeout_minutes': int(data.get('loss_of_signal_timeout_minutes', 30)),
        'enable_fermentation_starting': 'enable_fermentation_starting' in data,
        'enable_fermentation_completion': 'enable_fermentation_completion' in data,
        'enable_daily_report': 'enable_daily_report' in data,
        'daily_report_time': data.get('daily_report_time', '09:00'),
    }
    system_cfg['batch_notifications'] = batch_notif
    
    save_json(SYSTEM_CFG_FILE, system_cfg)

    new_warn = system_cfg.get('warning_mode','NONE')
    # Reset notification state when warning mode changes
    if old_warn.upper() == 'NONE' and new_warn.upper() in ('EMAIL','PUSH','BOTH'):
        temp_cfg['notifications_trigger'] = False
        temp_cfg['notification_comm_failure'] = False
    elif new_warn.upper() == 'NONE':
        temp_cfg['notifications_trigger'] = False
        temp_cfg['notification_comm_failure'] = False

    return redirect(f'/system_config?tab={active_tab}')

@app.route('/test_email', methods=['POST'])
def test_email():
    """Test email notification with current settings"""
    try:
        subject = "TEST - Fermenter Controller"
        body = "*** TEST MESSAGE ***\n\nThis is a TEST email from your Fermenter Temperature Controller.\n\nIf you received this, your email settings are configured correctly!\n\n*** TEST MESSAGE ***"
        
        success, error_msg = send_email(subject, body)
        
        if success:
            return jsonify({
                'success': True,
                'message': 'Test email sent successfully! Check your inbox.'
            })
        else:
            return jsonify({
                'success': False,
                'message': f'Failed to send test email: {error_msg}'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'An error occurred while sending test email: {str(e)}'
        })

@app.route('/test_push', methods=['POST'])
def test_push():
    """Test push notification with current settings"""
    try:
        # Determine which provider is configured
        push_provider = system_cfg.get("push_provider", "pushover").lower()
        provider_name = "Pushover" if push_provider == "pushover" else "ntfy"
        
        body = f"*** TEST MESSAGE *** This is a TEST push notification from your Fermenter Temperature Controller. If you received this, your {provider_name} settings are configured correctly! *** TEST MESSAGE ***"
        
        success, error_msg = send_push(body, subject="TEST - Fermenter Controller")
        
        if success:
            return jsonify({
                'success': True,
                'message': f'Test push notification sent successfully via {provider_name}! Check your device.'
            })
        else:
            return jsonify({
                'success': False,
                'message': f'Failed to send test push notification: {error_msg}'
            })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'An error occurred while sending test push notification: {str(e)}'
        })

@app.route('/test_external_logging', methods=['POST'])
def test_external_logging():
    """
    Test external logging connection with a test payload.
    
    Security Note: This endpoint intentionally makes requests to user-provided URLs
    to test external logging integrations. This is expected behavior for an admin
    configuration feature. Risk is mitigated by:
    - Admin-only access (system config page)
    - Timeout limits
    - No sensitive data in test payload
    - Controlled environment (Raspberry Pi)
    """
    try:
        data = request.get_json()
        index = data.get('index', 0)
        url = data.get('url', '').strip()
        
        if not url:
            return jsonify({
                'success': False,
                'message': 'No URL provided'
            })
        
        # Basic URL validation
        if not (url.startswith('http://') or url.startswith('https://')):
            return jsonify({
                'success': False,
                'message': 'URL must start with http:// or https://'
            })
        
        # Create a test payload
        test_payload = {
            "tilt_color": "TEST",
            "temp_f": 68.5,
            "gravity": 1.050,
            "brewid": "test_batch",
            "batch_name": "Test Connection",
            "beer_name": "Test Beer",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "test": True
        }
        
        # Get configuration settings from request (per-URL settings) or fall back to system config
        method = data.get('method', system_cfg.get("external_method", "POST")).upper()
        content_type = data.get('content_type', system_cfg.get("external_content_type", "form"))
        send_json = (content_type == "json")
        timeout = int(data.get('timeout_seconds', system_cfg.get("external_timeout_seconds", 8)) or 8)
        
        # Get field map settings
        field_map_id = data.get('field_map_id', 'default')
        custom_field_map = data.get('custom_field_map', '')
        
        # Helper function to apply field mapping
        def apply_field_mapping(payload, field_map):
            """Apply field mapping transformation to payload."""
            transformed = {}
            for logical_field, remote_field in field_map.items():
                if logical_field in payload:
                    transformed[remote_field] = payload[logical_field]
            return transformed
        
        # Transform for Brewers Friend if needed
        # Check if the URL contains brewersfriend.com as the domain (not in query params)
        # Note: Brewers Friend transformation takes precedence over custom field maps
        # to ensure compatibility with their API requirements
        try:
            parsed = urlparse(url)
            is_brewersfriend = 'brewersfriend.com' in parsed.netloc.lower()
        except Exception:
            # Fallback to simple string check if urlparse fails
            url_lower = url.lower()
            is_brewersfriend = url_lower.startswith('https://brewersfriend.com') or url_lower.startswith('http://brewersfriend.com')
        
        if is_brewersfriend:
            test_payload = {
                "name": "TEST",
                "temp": 68.5,
                "temp_unit": "F",
                "gravity": 1.050,
                "gravity_unit": "G",
                "beer": "Test Connection",
                "comment": "Test from Fermenter Controller"
            }
            send_json = True
        elif field_map_id and field_map_id != 'default':
            # Apply field map transformation if specified
            if field_map_id == 'custom' and custom_field_map:
                try:
                    field_map = json.loads(custom_field_map)
                    test_payload = apply_field_mapping(test_payload, field_map)
                except (json.JSONDecodeError, ValueError, TypeError) as e:
                    # If custom field map is invalid, use original payload
                    print(f"[WARNING] Invalid custom field map JSON for test connection: {e}")
                    # Continue with original payload instead of failing the test
            else:
                # Use predefined field map
                predefined_maps = get_predefined_field_maps()
                field_map = predefined_maps.get(field_map_id, {}).get("map")
                if field_map:
                    test_payload = apply_field_mapping(test_payload, field_map)
        
        # Attempt to send test data
        if requests is None:
            return jsonify({
                'success': False,
                'message': 'Requests library not available'
            })
        
        headers = {}
        
        try:
            if send_json:
                headers["Content-Type"] = "application/json"
                resp = requests.request(method, url, json=test_payload, headers=headers, timeout=timeout)
            else:
                headers["Content-Type"] = "application/x-www-form-urlencoded"
                # Build form data, converting None to empty string and filtering to simple types
                formdata = {}
                for k, v in test_payload.items():
                    if isinstance(v, (str, int, float, bool)) or v is None:
                        formdata[k] = "" if v is None else v
                resp = requests.request(method, url, data=formdata, headers=headers, timeout=timeout)
            
            # Check response
            if resp.status_code >= 200 and resp.status_code < 300:
                return jsonify({
                    'success': True,
                    'message': f'Connection successful! Status: {resp.status_code}'
                })
            else:
                # Don't expose response content in error messages for security
                return jsonify({
                    'success': False,
                    'message': f'Connection failed with HTTP status {resp.status_code}'
                })
        except requests.exceptions.Timeout:
            return jsonify({
                'success': False,
                'message': f'Connection timeout after {timeout} seconds'
            })
        except requests.exceptions.ConnectionError:
            return jsonify({
                'success': False,
                'message': 'Unable to connect to the specified URL. Please check the URL and network connection.'
            })
        except Exception:
            return jsonify({
                'success': False,
                'message': 'Request failed. Please check the URL and try again.'
            })
            
    except Exception:
        return jsonify({
            'success': False,
            'message': 'An error occurred while testing the connection. Please verify your settings and try again.'
        })

@app.route('/tilt_config', methods=['GET', 'POST'])
def tilt_config():
    selected = request.args.get('tilt_color') or request.form.get('tilt_color')
    batch_history = []
    if selected:
        try:
            with open(f'batches/batch_history_{selected}.json', 'r') as f:
                batch_history = json.load(f)
        except Exception:
            batch_history = []
    if request.method == 'POST':
        color = request.form.get('tilt_color')
        action = request.form.get('action')
        # --- PATCH: Capture quick OG/recipe/metadata changes as batch_metadata ----
        actual_og = request.form.get("actual_og")
        recipe_og = request.form.get("recipe_og")
        # update tilt_cfg fields from the form (for quick-edit path)
        changed = False
        if color in tilt_cfg:
            batch_entry = tilt_cfg[color].copy()
            if actual_og is not None:
                batch_entry['actual_og'] = actual_og
                tilt_cfg[color]['actual_og'] = actual_og
                changed = True
            if recipe_og is not None:
                batch_entry['recipe_og'] = recipe_og
                tilt_cfg[color]['recipe_og'] = recipe_og
                changed = True
            # Keep og_confirmed in data structure for backward compatibility (always False)
            batch_entry['og_confirmed'] = False
            tilt_cfg[color]['og_confirmed'] = False
            batch_entry['brewid'] = tilt_cfg[color].get("brewid")
            if changed:
                try:
                    save_json(TILT_CONFIG_FILE, tilt_cfg)
                except Exception:
                    pass
                # Append batch_metadata to batch file
                append_batch_metadata_to_batch_jsonl(color, batch_entry)
        if color and action:
            if action == "cancel":
                return redirect("/")
            return redirect(f"/batch_settings?tilt_color={color}&action={action}")
    config = tilt_cfg.get(selected, {}) if selected else {}
    return render_template('tilt_config.html',
        tilt_cfg=tilt_cfg,
        tilt_colors=list(TILT_UUIDS.values()),
        selected_tilt=selected,
        selected_config=config,
        system_settings=system_cfg,
        batch_history=batch_history
    )

@app.route('/batch_settings', methods=['GET', 'POST'])
def batch_settings():
    if request.method == 'POST':
        data = request.form
        color = data.get('tilt_color')
        if not color:
            return "No Tilt color selected", 400
        beer_name = data.get('beer_name', '').strip()
        batch_name = data.get('batch_name', '').strip()
        start_date = data.get('ferm_start_date', '').strip()
        existing = tilt_cfg.get(color, {})
        old_brew_id = existing.get('brewid')
        brew_id = existing.get('brewid')
        if not brew_id:
            brew_id = generate_brewid(beer_name, batch_name, start_date)
        if old_brew_id and old_brew_id != brew_id:
            rotate_and_archive_old_history(color, old_brew_id, existing)
            tilt_cfg[color] = {
                "beer_name": "",
                "batch_name": "",
                "ferm_start_date": "",
                "recipe_og": "",
                "recipe_fg": "",
                "recipe_abv": "",
                "actual_og": None,
                "brewid": "",
                "og_confirmed": False,
                "notification_state": {
                    "fermentation_start_datetime": None,
                    "fermentation_completion_datetime": None,
                    "last_daily_report": None
                }
            }
            save_json(TILT_CONFIG_FILE, tilt_cfg)

        og_confirmed = False  # No longer using og_confirmed checkbox

        batch_entry = {
            "beer_name": beer_name,
            "batch_name": batch_name,
            "ferm_start_date": start_date,
            "recipe_og": data.get('recipe_og', '') or '',
            "recipe_fg": data.get('recipe_fg', '') or '',
            "recipe_abv": data.get('recipe_abv', '') or '',
            "actual_og": (data.get('actual_og', '') or None),
            "og_confirmed": False,  # Keep field for backward compatibility
            "brewid": brew_id,
            "is_active": True,  # New field to track active vs closed batches
            "closed_date": None  # Track when batch was closed
        }
        
        # Preserve existing notification_state when editing a batch
        if color in tilt_cfg and 'notification_state' in tilt_cfg[color]:
            # Create a copy to avoid modifying the original
            batch_entry['notification_state'] = dict(tilt_cfg[color]['notification_state'])
        else:
            # Initialize notification_state for new batches
            batch_entry['notification_state'] = {
                "fermentation_start_datetime": None,
                "fermentation_completion_datetime": None,
                "last_daily_report": None
            }

        try:
            with open(f'batches/batch_history_{color}.json', 'r') as f:
                batches = json.load(f)
        except Exception:
            batches = []
        batches.append(batch_entry)
        try:
            with open(f'batches/batch_history_{color}.json', 'w') as f:
                json.dump(batches, f, indent=2)
        except Exception as e:
            print(f"[LOG] Could not append batch history for {color}: {e}")
        tilt_cfg[color] = batch_entry
        try:
            save_json(TILT_CONFIG_FILE, tilt_cfg)
        except Exception as e:
            print(f"[LOG] Could not save tilt_config in batch_settings: {e}")
        # --- PATCH: Append batch_metadata to .jsonl whenever batch is edited
        append_batch_metadata_to_batch_jsonl(color, batch_entry)
        return redirect(f"/batch_settings?tilt_color={color}")

    selected = request.args.get('tilt_color')
    action = request.args.get('action')
    config = tilt_cfg.get(selected, {}) if selected else {}
    batch_history = []
    if selected:
        try:
            with open(f'batches/batch_history_{selected}.json', 'r') as f:
                batch_history = json.load(f)
        except Exception:
            batch_history = []
    all_colors = list(TILT_UUIDS.values())
    active_tilts = get_active_tilts()
    active_colors = list(active_tilts.keys())
    return render_template('batch_settings.html',
        tilt_cfg=tilt_cfg,
        tilt_colors=all_colors,
        active_colors=active_colors,
        live_tilts=active_tilts,
        selected_tilt=selected,
        selected_config=config,
        system_settings=system_cfg,
        action=action,
        batch_history=batch_history,
        color_map=COLOR_MAP
    )

@app.route('/temp_config')
def temp_config():
    report_colors = list(tilt_cfg.keys())
    active_tilts = get_active_tilts()
    return render_template('temp_control_config.html',
        temp_control=temp_cfg,
        tilt_cfg=tilt_cfg,
        system_settings=system_cfg,
        batch_cfg=tilt_cfg,
        report_colors=report_colors,
        live_tilts=active_tilts
    )


@app.route('/update_temp_config', methods=['POST'])
def update_temp_config():
    data = request.form
    try:
        # Get the current and new tilt assignments
        old_tilt_color = temp_cfg.get("tilt_color", "")
        new_tilt_color = data.get('tilt_color', '')
        
        temp_cfg.update({
            "tilt_color": new_tilt_color,
            "low_limit": float(data.get('low_limit', 0)),
            "high_limit": float(data.get('high_limit', 100)),
            "enable_heating": 'enable_heating' in data,
            "enable_cooling": 'enable_cooling' in data,
            "heating_plug": data.get("heating_plug", ""),
            "cooling_plug": data.get("cooling_plug", ""),
            "mode": data.get("mode", temp_cfg.get('mode','')),
            "status": data.get("status", temp_cfg.get('status',''))
        })
        
        # If a new Tilt is being assigned (or changed), record the assignment time
        # This starts the grace period for the newly assigned Tilt
        if new_tilt_color and new_tilt_color != old_tilt_color:
            from datetime import datetime
            temp_cfg["tilt_assignment_time"] = datetime.utcnow().isoformat()
            print(f"[TEMP_CONTROL] Tilt '{new_tilt_color}' assigned to temperature control - starting 15-minute grace period")
        elif not new_tilt_color and old_tilt_color:
            # Tilt was unassigned - clear the assignment time
            temp_cfg.pop("tilt_assignment_time", None)
            print(f"[TEMP_CONTROL] Tilt unassigned from temperature control")
            
    except Exception as e:
        print(f"[LOG] Error parsing temp config form: {e}")
    try:
        save_json(TEMP_CFG_FILE, temp_cfg)
    except Exception as e:
        print(f"[LOG] Error saving config in update_temp_config: {e}")


    # Run control logic immediately (it will normalize mode/status and log selection change if any)
    temperature_control_logic()


    return redirect('/temp_config')


@app.route('/toggle_temp_control', methods=['POST'])
def toggle_temp_control():
    """Toggle the temp_control_active state (ON/OFF switch on temp control card).
    
    When turning ON, if 'new_session' is True in the request, archive the existing log.
    """
    try:
        data = request.get_json() if request.is_json else request.form
        # Standardize on boolean JSON values
        active_value = data.get('active')
        if isinstance(active_value, bool):
            new_state = active_value
        elif isinstance(active_value, str):
            new_state = active_value.lower() in ('true', '1')
        else:
            new_state = bool(active_value)
        
        # Check if this is a new session request (archive existing log)
        new_session = data.get('new_session', False)
        if isinstance(new_session, str):
            new_session = new_session.lower() in ('true', '1')
        
        # If turning ON and new_session is requested, archive the existing log
        if new_state and new_session:
            try:
                if os.path.exists(LOG_PATH):
                    # Create logs directory if it doesn't exist
                    logs_dir = 'logs'
                    os.makedirs(logs_dir, exist_ok=True)
                    
                    # Generate archive filename with timestamp
                    timestamp = datetime.utcnow().strftime('%Y%m%d_%H%M%S')
                    tilt_color = temp_cfg.get("tilt_color", "unknown")
                    archive_name = f"temp_control_{tilt_color}_{timestamp}.jsonl"
                    archive_path = os.path.join(logs_dir, archive_name)
                    
                    # Move the existing log to archive
                    shutil.move(LOG_PATH, archive_path)
                    print(f"[LOG] Archived temp control log to {archive_path}")
            except Exception as e:
                print(f"[LOG] Error archiving temp control log: {e}")
                return jsonify({"success": False, "error": f"Failed to archive log: {str(e)}"}), 500
        
        temp_cfg['temp_control_active'] = new_state
        
        if new_state:
            # When turning ON, arm all triggers and log the start event
            temp_cfg['in_range_trigger_armed'] = True
            temp_cfg['above_limit_trigger_armed'] = True
            temp_cfg['below_limit_trigger_armed'] = True
            append_control_log("temp_control_started", {
                "low_limit": temp_cfg.get("low_limit"),
                "current_temp": temp_cfg.get("current_temp"),
                "high_limit": temp_cfg.get("high_limit"),
                "tilt_color": temp_cfg.get("tilt_color", "")
            })
        
        # Save the state
        save_json(TEMP_CFG_FILE, temp_cfg)
        
        # If this was a new session, signal that we should redirect to temp_config
        return jsonify({"success": True, "active": new_state, "redirect": "/temp_config" if (new_state and new_session) else None})
    except Exception as e:
        print(f"[LOG] Error toggling temp control: {e}")
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/temp_report', methods=['GET', 'POST'])
def temp_report():
    if request.method == 'POST':
        color = request.form.get('tilt_color')
        if not color:
            return redirect('/temp_report')
        return redirect(f"/temp_report?tilt_color={color}&page=1")


    tilt_color = request.args.get('tilt_color')
    try:
        page = int(request.args.get('page', '1'))
    except Exception:
        page = 1


    if not tilt_color:
        colors = list(tilt_cfg.keys())
        default_color = colors[0] if colors else None
        return render_template('temp_report_select.html', colors=colors, default_color=default_color)


    entries = []
    try:
        if os.path.exists(LOG_PATH):
            with open(LOG_PATH, 'r') as f:
                for line in f:
                    try:
                        obj = json.loads(line)
                    except Exception:
                        continue
                    if obj.get('event') != 'tilt_reading' and obj.get('event') != 'SAMPLE':
                        continue
                    payload = obj.get('payload') or obj if isinstance(obj, dict) else {}
                    entries.append(payload)
    except Exception as e:
        print(f"[LOG] Could not read log for temp_report: {e}")


    filtered = []
    brewid = tilt_cfg.get(tilt_color, {}).get('brewid')
    tc = tilt_cfg.get(tilt_color, {}) or {}
    for p in entries:
        if brewid:
            if p.get('brewid') == brewid:
                filtered.append(p)
        else:
            if p.get('batch_name') == tc.get('batch_name') or p.get('beer_name') == tc.get('beer_name'):
                filtered.append(p)


    lines = []
    filtered = list(reversed(filtered))
    for p in filtered:
        ts = p.get('timestamp', '')
        bn = p.get('beer_name') or ''
        batch = p.get('batch_name') or ''
        tempf = p.get('temp_f', '')
        grav = p.get('gravity', '')
        bid = p.get('brewid') or '--'
        lines.append(f"{ts} — {bn or batch} — Temp: {tempf}°F — Gravity: {grav} — Brew ID: {bid}")


    total_pages = max(1, ceil(len(lines) / PER_PAGE))
    page = max(1, min(page, total_pages))
    start = (page - 1) * PER_PAGE
    end = start + PER_PAGE
    page_data = lines[start:end]
    at_end = page >= total_pages


    return render_template('temp_report_display.html',
                           color=tilt_color,
                           page=page,
                           total_pages=total_pages,
                           page_data=page_data,
                           at_end=at_end)


@app.route('/batch_history')
def batch_history():
    """
    Display batch history selection page.
    
    Shows two sections:
    1. Current Activity - all active batches (is_active=True), sorted by user criteria
    2. Batch History - all closed batches (is_active=False), sorted by user criteria
    
    Archive location: batches/archive/ directory
    """
    # Get sort order from query parameter (default: newest first)
    sort_order = request.args.get('sort', 'newest')
    
    active_batches = []
    closed_batches = []
    
    # Check each color for batch history
    for color in TILT_UUIDS.values():
        batch_history_file = f'batches/batch_history_{color}.json'
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
                    for batch in batches:
                        # Add color to batch for display
                        batch['color'] = color
                        # Migrate old batches without is_active field (default to active)
                        if 'is_active' not in batch:
                            batch['is_active'] = True
                        
                        if batch.get('is_active', True):
                            active_batches.append(batch)
                        else:
                            closed_batches.append(batch)
            except Exception as e:
                print(f"[LOG] Error loading batch history for {color}: {e}")
    
    # Define sorting function
    def apply_sort(batches, sort_order):
        if sort_order == 'newest':
            return sorted(batches, key=lambda x: x.get('ferm_start_date', ''), reverse=True)
        elif sort_order == 'oldest':
            return sorted(batches, key=lambda x: x.get('ferm_start_date', ''))
        elif sort_order == 'beer_name':
            return sorted(batches, key=lambda x: (x.get('beer_name', '').lower(), x.get('ferm_start_date', '')))
        elif sort_order == 'color':
            return sorted(batches, key=lambda x: (x.get('color', ''), x.get('ferm_start_date', '')), reverse=True)
        return batches
    
    # Apply sorting to both sections
    active_batches = apply_sort(active_batches, sort_order)
    closed_batches = apply_sort(closed_batches, sort_order)
    
    return render_template('batch_history_select.html',
                         active_batches=active_batches,
                         closed_batches=closed_batches,
                         color_map=COLOR_MAP,
                         sort_order=sort_order)


@app.route('/batch_review/<brewid>')
def batch_review(brewid):
    """Display detailed review of a specific batch by brewid."""
    # Sanitize brewid to prevent directory traversal attacks
    # Only allow alphanumeric characters and hyphens
    import re
    if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
        return "Invalid batch ID", 400
    
    # Find the batch in batch_history files
    batch_info = None
    color = None
    
    for c in TILT_UUIDS.values():
        batch_history_file = f'batches/batch_history_{c}.json'
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
                    for b in batches:
                        if b.get('brewid') == brewid:
                            batch_info = b
                            color = c
                            break
            except Exception:
                pass
        if batch_info:
            break
    
    if not batch_info:
        return "Batch not found", 404
    
    # Load batch data from JSONL file
    batch_data = []
    batch_file = None
    
    # Check with glob pattern for files matching the brewid (now sanitized)
    batch_files = glob_func(f'batches/*{brewid}*.jsonl')
    
    if batch_files:
        batch_file = batch_files[0]
        try:
            with open(batch_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            entry = json.loads(line)
                            batch_data.append(entry)
                        except Exception:
                            pass
        except Exception:
            pass
    
    # Calculate statistics from batch data
    stats = calculate_batch_statistics(batch_data, batch_info)
    
    return render_template('batch_review.html',
                         batch=batch_info,
                         color=color,
                         batch_data=batch_data,
                         stats=stats,
                         color_map=COLOR_MAP)


def calculate_batch_statistics(batch_data, batch_info):
    """
    Calculate statistics from batch data.
    
    Issue 5 fix: Ensures statistics use the FULL data point set from batch_data.
    """
    # Initialize stats - total_readings counts ALL entries, not just samples
    all_samples = []
    for entry in batch_data:
        if entry.get('event') in ['sample', 'SAMPLE', 'tilt_reading']:
            payload = entry.get('payload', entry)
            all_samples.append(payload)
    
    stats = {
        'total_readings': len(all_samples),  # Count actual sample entries
        'duration_days': None,
        'start_gravity': None,
        'end_gravity': None,
        'gravity_change': None,
        'start_temp': None,
        'end_temp': None,
        'avg_temp': None,
        'min_temp': None,
        'max_temp': None,
        'estimated_abv': None
    }
    
    if not all_samples:
        return stats
    
    # Calculate temperature statistics from ALL samples
    temps = [s.get('temp_f') for s in all_samples if s.get('temp_f') is not None]
    if temps:
        stats['avg_temp'] = round(sum(temps) / len(temps), 1)
        stats['min_temp'] = min(temps)
        stats['max_temp'] = max(temps)
        stats['start_temp'] = temps[0]
        stats['end_temp'] = temps[-1]
    
    # Calculate gravity statistics from ALL samples
    gravities = [s.get('gravity') for s in all_samples if s.get('gravity') is not None]
    if gravities:
        stats['start_gravity'] = gravities[0]
        stats['end_gravity'] = gravities[-1]
        stats['gravity_change'] = round(gravities[0] - gravities[-1], 3)
    
    # Calculate ABV if we have actual_og
    actual_og = batch_info.get('actual_og')
    if actual_og and gravities:
        try:
            og_float = float(actual_og)
            final_gravity = gravities[-1]
            # ABV = (OG - FG) * 131.25
            stats['estimated_abv'] = round((og_float - final_gravity) * 131.25, 2)
        except (ValueError, TypeError):
            pass
    
    # Calculate duration from ALL timestamps
    timestamps = [s.get('timestamp') for s in all_samples if s.get('timestamp')]
    if len(timestamps) >= 2:
        try:
            start_time = datetime.fromisoformat(timestamps[0].replace('Z', '+00:00'))
            end_time = datetime.fromisoformat(timestamps[-1].replace('Z', '+00:00'))
            duration = end_time - start_time
            stats['duration_days'] = duration.days
        except Exception:
            pass
    
    return stats


@app.route('/batch_data_view/<brewid>')
def batch_data_view(brewid):
    """
    View all batch data with optional range selection.
    Allows viewing a subset of data points by start/end parameters.
    """
    import re
    if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
        return "Invalid batch ID", 400
    
    # Find the batch in batch_history files
    batch_info = None
    color = None
    
    for c in TILT_UUIDS.values():
        batch_history_file = f'batches/batch_history_{c}.json'
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
                    for b in batches:
                        if b.get('brewid') == brewid:
                            batch_info = b
                            color = c
                            break
            except Exception:
                pass
        if batch_info:
            break
    
    if not batch_info:
        return "Batch not found", 404
    
    # Load batch data from JSONL file
    batch_data = []
    batch_files = glob_func(f'batches/*{brewid}*.jsonl')
    
    if batch_files:
        batch_file = batch_files[0]
        try:
            with open(batch_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            entry = json.loads(line)
                            batch_data.append(entry)
                        except Exception:
                            pass
        except Exception:
            pass
    
    # Extract samples
    all_samples = []
    for entry in batch_data:
        if entry.get('event') in ['sample', 'SAMPLE', 'tilt_reading']:
            payload = entry.get('payload', entry)
            all_samples.append(payload)
    
    # Apply range filter if specified
    start_idx = request.args.get('start', type=int)
    end_idx = request.args.get('end', type=int)
    
    if start_idx is not None or end_idx is not None:
        # Convert to 0-based index
        start = (start_idx - 1) if start_idx else 0
        end = end_idx if end_idx else len(all_samples)
        all_samples = all_samples[start:end]
    
    # Calculate statistics
    stats = calculate_batch_statistics(batch_data, batch_info)
    
    return render_template('batch_data_view.html',
                         batch=batch_info,
                         color=color,
                         samples=all_samples,
                         stats=stats,
                         color_map=COLOR_MAP,
                         start_idx=start_idx,
                         end_idx=end_idx)


@app.route('/export_batch_data_csv/<brewid>')
def export_batch_data_csv(brewid):
    """
    Export batch data to CSV with optional range selection.
    """
    import csv
    import re
    
    if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
        return "Invalid batch ID", 400
    
    # Find the batch
    batch_info = None
    color = None
    
    for c in TILT_UUIDS.values():
        batch_history_file = f'batches/batch_history_{c}.json'
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
                    for b in batches:
                        if b.get('brewid') == brewid:
                            batch_info = b
                            color = c
                            break
            except Exception:
                pass
        if batch_info:
            break
    
    if not batch_info:
        return "Batch not found", 404
    
    # Load batch data
    batch_data = []
    batch_files = glob_func(f'batches/*{brewid}*.jsonl')
    
    if batch_files:
        batch_file = batch_files[0]
        try:
            with open(batch_file, 'r') as f:
                for line in f:
                    if line.strip():
                        try:
                            entry = json.loads(line)
                            batch_data.append(entry)
                        except Exception:
                            pass
        except Exception:
            pass
    
    # Extract samples
    all_samples = []
    for entry in batch_data:
        if entry.get('event') in ['sample', 'SAMPLE', 'tilt_reading']:
            payload = entry.get('payload', entry)
            all_samples.append(payload)
    
    # Apply range filter if specified
    start_idx = request.args.get('start', type=int)
    end_idx = request.args.get('end', type=int)
    
    if start_idx is not None or end_idx is not None:
        start = (start_idx - 1) if start_idx else 0
        end = end_idx if end_idx else len(all_samples)
        all_samples = all_samples[start:end]
    
    # Create CSV export
    export_dir = 'export'
    os.makedirs(export_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    beer_name = batch_info.get('beer_name', 'batch').replace(' ', '_')
    filename = f'{beer_name}_{brewid[:8]}_{timestamp}.csv'
    filepath = os.path.join(export_dir, filename)
    
    try:
        with open(filepath, 'w', newline='') as csvfile:
            fieldnames = ['timestamp', 'tilt_color', 'gravity', 'temp_f', 'rssi', 'beer_name', 'batch_name']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
            
            writer.writeheader()
            for sample in all_samples:
                writer.writerow(sample)
        
        # Send file for download
        from flask import send_file
        return send_file(filepath, as_attachment=True, download_name=filename)
    except Exception as e:
        return f"Error exporting CSV: {str(e)}", 500


@app.route('/export_temp_log', methods=['GET', 'POST'])
def export_temp_log():
    return redirect('/temp_config')


@app.route('/export_temp_csv', methods=['GET', 'POST'])
def export_temp_csv():
    return redirect('/temp_config')


@app.route('/export_temp_control_csv', methods=['POST'])
def export_temp_control_csv():
    """Export temperature control log data to CSV in the /export directory."""
    try:
        import csv
        from datetime import datetime
        
        # Create export directory if it doesn't exist
        export_dir = 'export'
        os.makedirs(export_dir, exist_ok=True)
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f'temp_control_{timestamp}.csv'
        filepath = os.path.join(export_dir, filename)
        
        # Read data from temp_control_log.jsonl
        data_rows = []
        if os.path.exists(LOG_PATH):
            with open(LOG_PATH, 'r') as f:
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        obj = json.loads(line)
                        # Only include events that are in ALLOWED_EVENTS
                        if obj.get('event') in ALLOWED_EVENT_VALUES:
                            data_rows.append(obj)
                    except Exception as e:
                        print(f"[LOG] Error parsing line in export: {e}")
                        continue
        
        # Write to CSV
        if data_rows:
            with open(filepath, 'w', newline='') as csvfile:
                # Define fieldnames from the data
                fieldnames = ['timestamp', 'date', 'time', 'tilt_color', 'brewid', 'low_limit', 'current_temp', 'temp_f', 'gravity', 'high_limit', 'event']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()
                for row in data_rows:
                    writer.writerow(row)
            
            return jsonify({'success': True, 'filename': filename, 'rows': len(data_rows)})
        else:
            return jsonify({'success': False, 'error': 'No data to export'})
            
    except Exception as e:
        print(f"[LOG] Error exporting temp control CSV: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/scan_kasa_plugs')
def scan_kasa_plugs():
    try:
        from kasa import Discover
        found_devices = asyncio.run(Discover.discover())
        devices = {str(addr): dev.alias for addr, dev in found_devices.items()}
    except Exception as e:
        devices = {}
        print(f"[LOG] Kasa scan failed: {e}")
    return render_template("kasa_scan_results.html", devices=devices, error=None)


def format_kasa_error(error_msg, device_url):
    """Format KASA error messages to be more user-friendly"""
    error_str = str(error_msg)
    
    # Check if this is a localhost address - this is a common configuration mistake
    if device_url.startswith('127.') or device_url == 'localhost':
        return f"❌ Invalid IP address: {device_url} is a localhost address. KASA plugs require a real network IP address (typically 192.168.x.x or 10.0.x.x). Check your router's DHCP client list or use the Kasa mobile app to find the plug's actual IP address."
    
    # Connection refused errors (port closed, device not listening)
    if 'Errno 111' in error_str or 'Connect call failed' in error_str or 'Connection refused' in error_str:
        return f"Cannot connect to device. Please check: (1) the device is powered on, (2) the IP address {device_url} is correct, (3) the device is on the same network"
    
    # Timeout errors
    if 'TimeoutError' in error_str or 'timed out' in error_str.lower():
        return f"Connection timed out. The device may be unreachable or turned off"
    
    # Host unreachable
    if 'Errno 113' in error_str or 'No route to host' in error_str:
        return f"Network error: No route to {device_url}. Check network configuration"
    
    # Name resolution errors
    if 'Name or service not known' in error_str or 'getaddrinfo failed' in error_str:
        return f"Cannot resolve hostname: {device_url}. Use an IP address instead"
    
    # Permission errors
    if 'Errno 13' in error_str or 'Permission denied' in error_str:
        return "Permission denied. Network configuration issue"
    
    # Default: return a simplified version
    # Try to extract the most relevant part
    if 'Unable to connect' in error_str:
        # Already formatted nicely by kasa library
        return error_str.split('\n')[0]  # Just first line
    
    return error_str


@app.route('/test_kasa_plugs', methods=['POST'])
def test_kasa_plugs():
    """Test connection to configured KASA plugs"""
    try:
        data = request.get_json()
        heating_url = data.get('heating_url', '').strip()
        cooling_url = data.get('cooling_url', '').strip()
        
        results = {}
        
        # Import once at function level
        try:
            from kasa.iot import IotPlug
        except ImportError:
            return jsonify({'error': 'KASA library not available'}), 500
        
        # Test heating plug if URL is provided
        if heating_url:
            try:
                plug = IotPlug(heating_url)
                asyncio.run(asyncio.wait_for(plug.update(), timeout=6))
                results['heating'] = {'success': True, 'error': None}
            except Exception as e:
                results['heating'] = {'success': False, 'error': format_kasa_error(e, heating_url)}
        
        # Test cooling plug if URL is provided
        if cooling_url:
            try:
                plug = IotPlug(cooling_url)
                asyncio.run(asyncio.wait_for(plug.update(), timeout=6))
                results['cooling'] = {'success': True, 'error': None}
            except Exception as e:
                results['cooling'] = {'success': False, 'error': format_kasa_error(e, cooling_url)}
        
        return jsonify(results)
    except Exception as e:
        print(f"[LOG] Error testing KASA plugs: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/live_snapshot')
def live_snapshot():
    snapshot = {
        "live_tilts": {},
        "temp_control": {
            "current_temp": temp_cfg.get("current_temp"),
            "low_limit": temp_cfg.get("low_limit"),
            "high_limit": temp_cfg.get("high_limit"),
            "tilt_color": temp_cfg.get("tilt_color", ""),
            "tilt_color_code": COLOR_MAP.get(temp_cfg.get("tilt_color", ""), ""),
            "heater_on": temp_cfg.get("heater_on"),
            "cooler_on": temp_cfg.get("cooler_on"),
            "heater_pending": temp_cfg.get("heater_pending"),
            "cooler_pending": temp_cfg.get("cooler_pending"),
            "enable_heating": temp_cfg.get("enable_heating"),
            "enable_cooling": temp_cfg.get("enable_cooling"),
            "status": temp_cfg.get("status"),
            "mode": temp_cfg.get("mode", 'Off'),
            "warning_mode": system_cfg.get('warning_mode'),
            "notifications_trigger": temp_cfg.get('notifications_trigger'),
            "notification_comm_failure": temp_cfg.get('notification_comm_failure'),
            "temp_control_active": temp_cfg.get('temp_control_active', False),
            "heating_error": temp_cfg.get('heating_error', False),
            "cooling_error": temp_cfg.get('cooling_error', False),
            "push_error": temp_cfg.get('push_error', False),
            "email_error": temp_cfg.get('email_error', False)
        }
    }
    # Only include active tilts (those that have sent data recently)
    active_tilts = get_active_tilts()
    for color, info in active_tilts.items():
        snapshot["live_tilts"][color] = {
            "gravity": info.get("gravity"),
            "temp_f": info.get("temp_f"),
            "timestamp": info.get("timestamp"),
            "beer_name": info.get("beer_name"),
            "batch_name": info.get("batch_name"),
            "brewid": info.get("brewid"),
            "recipe_og": info.get("recipe_og"),
            "recipe_fg": info.get("recipe_fg"),
            "recipe_abv": info.get("recipe_abv"),
            "actual_og": info.get("actual_og"),
            "og_confirmed": info.get("og_confirmed", False),
            "original_gravity": info.get("original_gravity"),
            "color_code": info.get("color_code")
        }
    return jsonify(snapshot)


# --- Chart routes and data endpoint ---------------------------------------
@app.route('/chart_plotly')
def chart_plotly_index():
    colors = list(tilt_cfg.keys())
    if colors:
        return redirect(f'/chart_plotly/{colors[0]}')
    return render_template('chart_plotly.html', tilt_color=None, system_settings=system_cfg)


@app.route('/chart_plotly/<tilt_color>')
def chart_plotly_for(tilt_color):
    # Allow "Fermenter" as a special identifier for temperature control
    if tilt_color and tilt_color != "Fermenter" and tilt_color not in tilt_cfg:
        abort(404)
    return render_template(
        'chart_plotly.html',
        tilt_color=tilt_color,
        tilt_cfg=tilt_cfg,
        system_settings=system_cfg
    )

@app.route('/chart_data/<tilt_color>')
def chart_data_for(tilt_color):
    """
    Retrieve chart data for a specific tilt color or temperature control.
    
    Args:
        tilt_color: Tilt color name (e.g., 'Black', 'Blue') or 'Fermenter' for temperature control
        
    Query Parameters:
        all: If '1', 'true', 'yes', or 'on', return all available data
        limit: Maximum number of data points to return (default: DEFAULT_CHART_LIMIT)
        
    Returns:
        JSON object with:
            - points: Array of data points with timestamp, temp_f, and gravity (for tilts) or event (for temp control)
            - truncated: Boolean indicating if data was truncated due to limit
            - matched: Total number of matching entries found
            
    Data Sources:
        - Tilt colors: Read from batch-specific JSONL files in batches/ directory
        - 'Fermenter': Read from temp_control_log.jsonl
        
    Supported Formats:
        - Event/payload format: {"event": "sample", "payload": {...}}
        - Legacy direct object format: {"timestamp": ..., "gravity": ..., "temp_f": ...}
    """
    all_flag = str(request.args.get('all', '')).lower() in ('1', 'true', 'yes', 'on')
    limit_param = request.args.get('limit', None)
    limit = None
    if limit_param:
        try:
            limit = int(limit_param)
        except Exception:
            limit = None
    if not all_flag and (limit is None or limit <= 0):
        limit = DEFAULT_CHART_LIMIT
    if not all_flag and limit is not None:
        limit = max(10, min(limit, MAX_CHART_LIMIT))

    # Handle "Fermenter" as temperature control monitor data
    if tilt_color == "Fermenter":
        points = deque(maxlen=limit) if (not all_flag and limit is not None) else []
        matched = 0
        
        if os.path.exists(LOG_PATH):
            try:
                with open(LOG_PATH, 'r') as f:
                    for line in f:
                        if not line:
                            continue
                        try:
                            obj = json.loads(line)
                        except Exception:
                            continue
                        # Include all temp control events
                        event = obj.get('event', '')
                        if event not in ALLOWED_EVENT_VALUES:
                            continue
                        
                        matched += 1
                        ts = obj.get('timestamp')
                        tf = obj.get('temp_f') if obj.get('temp_f') is not None else obj.get('current_temp')
                        g = obj.get('gravity')
                        
                        try:
                            ts_str = str(ts) if ts is not None else None
                        except Exception:
                            ts_str = None
                        try:
                            temp_num = float(tf) if (tf is not None and tf != '') else None
                        except Exception:
                            temp_num = None
                        try:
                            grav_num = float(g) if (g is not None and g != '') else None
                        except Exception:
                            grav_num = None
                        
                        entry = {"timestamp": ts_str, "temp_f": temp_num, "gravity": grav_num, "event": event, "tilt_color": obj.get('tilt_color', '')}
                        if isinstance(points, deque):
                            points.append(entry)
                        else:
                            points.append(entry)
                            if len(points) > MAX_ALL_LIMIT:
                                points.pop(0)
            except Exception as e:
                print(f"[LOG] Error reading temp control log for chart_data: {e}")
        
        if isinstance(points, deque):
            pts = list(points)
            truncated = (matched > len(pts))
        else:
            pts = list(points)
            truncated = (matched > len(pts))
        return jsonify({"tilt_color": tilt_color, "points": pts, "truncated": truncated, "matched": matched})

    # Original tilt color logic
    if tilt_color and tilt_color not in tilt_cfg:
        return jsonify({"tilt_color": tilt_color, "points": [], "truncated": False, "matched": 0})

    brewid = tilt_cfg.get(tilt_color, {}).get('brewid')
    
    # Sanitize brewid to prevent directory traversal attacks
    # Only allow alphanumeric characters, hyphens, and underscores
    import re
    if brewid and not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
        print(f"[LOG] Invalid brewid format for {tilt_color}: {brewid}")
        return jsonify({"tilt_color": tilt_color, "points": [], "truncated": False, "matched": 0})
    
    points = deque(maxlen=limit) if (not all_flag and limit is not None) else []
    matched = 0
    
    # Find batch file(s) for this brewid
    batch_files = []
    if brewid:
        batch_files = glob_func(f'batches/*{brewid}*.jsonl')
        # Sort batch files by name for consistent ordering
        batch_files.sort()
    
    if batch_files:
        # Read from batch file(s)
        for batch_file in batch_files:
            try:
                with open(batch_file, 'r') as f:
                    for line in f:
                        if not line.strip():
                            continue
                        try:
                            obj = json.loads(line)
                        except Exception:
                            continue
                        
                        # Handle both event/payload format and direct object format
                        # New format: {"event": "sample", "payload": {...}}
                        # Legacy format: {"timestamp": ..., "gravity": ..., "temp_f": ...}
                        if obj.get('event') == 'sample':
                            payload = obj.get('payload', {})
                        elif obj.get('event') == 'batch_metadata':
                            # Skip metadata entries
                            continue
                        elif 'timestamp' in obj or 'gravity' in obj or 'temp_f' in obj:
                            # Legacy format - direct object
                            payload = obj
                        else:
                            # Unknown format, skip
                            continue
                        
                        matched += 1
                        ts = payload.get('timestamp')
                        tf = payload.get('temp_f') if payload.get('temp_f') is not None else payload.get('current_temp')
                        g = payload.get('gravity')
                        
                        try:
                            ts_str = str(ts) if ts is not None else None
                        except Exception:
                            ts_str = None
                        try:
                            temp_num = float(tf) if (tf is not None and tf != '') else None
                        except Exception:
                            temp_num = None
                        try:
                            grav_num = float(g) if (g is not None and g != '') else None
                        except Exception:
                            grav_num = None
                        
                        entry = {"timestamp": ts_str, "temp_f": temp_num, "gravity": grav_num}
                        if isinstance(points, deque):
                            points.append(entry)
                        else:
                            points.append(entry)
                            if len(points) > MAX_ALL_LIMIT:
                                points.pop(0)
            except Exception as e:
                print(f"[LOG] Error reading batch file {batch_file} for chart_data: {e}")
    
    if isinstance(points, deque):
        pts = list(points)
        truncated = (matched > len(pts))
    else:
        pts = list(points)
        truncated = (matched > len(pts))
    return jsonify({"tilt_color": tilt_color, "points": pts, "truncated": truncated, "matched": matched})


# --- Reset logs endpoint ---------------------------------------------------
@app.route('/reset_logs', methods=['POST'])
def reset_logs():
    """
    Reset (clear) the main temp_control_log.jsonl file after backing it up.
    """
    try:
        if os.path.exists(LOG_PATH):
            backup_name = f"{LOG_PATH}.{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.bak"
            try:
                os.rename(LOG_PATH, backup_name)
                append_control_log("temp_control_mode_changed", {"low_limit": temp_cfg.get("low_limit"), "current_temp": temp_cfg.get("current_temp"), "high_limit": temp_cfg.get("high_limit"), "tilt_color": temp_cfg.get("tilt_color", "")})
            except Exception as e:
                print(f"[LOG] Could not backup log: {e}")
        open(LOG_PATH, 'w').close()
        return redirect('/temp_config')
    except Exception as e:
        print(f"[LOG] reset_logs error: {e}")
        return "Error resetting logs", 500


# --- Misc UI routes -------------------------------------------------------
# Note: /export_temp_csv is already defined at line 3798


# --- Log Management Routes ------------------------------------------------
@app.route('/log_management')
def log_management():
    """Display the log and data management page."""
    try:
        # Get temp control log info
        temp_log_size = "0 bytes"
        if os.path.exists(LOG_PATH):
            size_bytes = os.path.getsize(LOG_PATH)
            temp_log_size = _format_file_size(size_bytes)
        
        # Get application logs
        app_logs = []
        log_dir = 'logs'
        if os.path.exists(log_dir):
            for filename in os.listdir(log_dir):
                if filename.endswith('.log') and filename != '.gitkeep':
                    filepath = os.path.join(log_dir, filename)
                    size_bytes = os.path.getsize(filepath)
                    app_logs.append({
                        'name': filename,
                        'size': _format_file_size(size_bytes),
                        'path': filepath
                    })
        
        # Get batch files
        batches = []
        if os.path.exists(BATCHES_DIR):
            for filename in os.listdir(BATCHES_DIR):
                if filename.endswith('.jsonl') and not filename.endswith('.backup'):
                    filepath = os.path.join(BATCHES_DIR, filename)
                    size_bytes = os.path.getsize(filepath)
                    
                    # Try to extract brewid from filename
                    brewid = filename.replace('.jsonl', '')
                    
                    # Try to get batch info from tilt_cfg
                    beer_name = None
                    ferm_start_date = None
                    for color, cfg in tilt_cfg.items():
                        if cfg.get('brewid') == brewid:
                            beer_name = cfg.get('beer_name')
                            ferm_start_date = cfg.get('ferm_start_date')
                            break
                    
                    batches.append({
                        'filename': filename,
                        'brewid': brewid,
                        'beer_name': beer_name,
                        'ferm_start_date': ferm_start_date,
                        'size': _format_file_size(size_bytes)
                    })
        
        # Sort batches by fermentation start date (most recent first)
        # Batches without a date or invalid date format appear at the end
        def sort_key(batch):
            date_str = batch.get('ferm_start_date')
            if date_str:
                try:
                    # Parse date string (format: YYYY-MM-DD)
                    return datetime.strptime(date_str, '%Y-%m-%d')
                except (ValueError, TypeError) as e:
                    # Log invalid date format for debugging
                    print(f"[LOG] Invalid date format for batch {batch.get('brewid')}: {date_str}")
            # Return a very old date for batches without dates or invalid dates
            return datetime(1900, 1, 1)
        
        batches.sort(key=sort_key, reverse=True)
        
        return render_template('log_management.html',
                             temp_log_size=temp_log_size,
                             app_logs=app_logs,
                             batches=batches,
                             success_message=request.args.get('success'),
                             error_message=request.args.get('error'))
    except Exception as e:
        print(f"[LOG] Error in log_management: {e}")
        return "Error loading log management page", 500


def _format_file_size(size_bytes):
    """Format file size in human-readable format."""
    for unit in ['bytes', 'KB', 'MB', 'GB']:
        if size_bytes < 1024.0:
            return f"{size_bytes:.1f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.1f} TB"


@app.route('/view_log')
def view_log():
    """Display the content of a log file."""
    try:
        log_file = request.args.get('file')
        log_type = request.args.get('type', 'app')  # 'app' or 'temp'
        
        if not log_file:
            return "No log file specified", 400
        
        # Security: validate log file path
        if log_type == 'temp':
            # Temperature control log
            if log_file != 'temp_control_log.jsonl':
                return "Invalid log file", 400
            filepath = LOG_PATH
        else:
            # Application log - restrict to alphanumeric, dash, underscore, single dot before .log
            if not re.match(r'^[a-zA-Z0-9\-_]+\.log$', log_file):
                return "Invalid log file name", 400
            filepath = os.path.join('logs', log_file)
        
        if not os.path.exists(filepath):
            return f"Log file not found: {log_file}", 404
        
        # Read the last 1000 lines efficiently using deque
        lines = deque(maxlen=1000)
        try:
            with open(filepath, 'r') as f:
                for line in f:
                    lines.append(line)
        except Exception as e:
            return f"Error reading log file: {str(e)}", 500
        
        content = ''.join(lines)
        
        return render_template('view_log.html',
                             log_file=log_file,
                             log_type=log_type,
                             content=content,
                             line_count=len(lines))
    except Exception as e:
        print(f"[LOG] Error viewing log: {e}")
        return f"Error viewing log: {str(e)}", 500


@app.route('/archive_temp_log', methods=['POST'])
def archive_temp_log():
    """Archive and reset the temperature control log."""
    try:
        if os.path.exists(LOG_PATH):
            # Create backup with timestamp
            backup_name = f"{LOG_PATH}.{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.bak"
            shutil.copy2(LOG_PATH, backup_name)
            print(f"[LOG] Archived temp control log to: {backup_name}")
            
            # Reset the log file
            open(LOG_PATH, 'w').close()
            
            return redirect(url_for('log_management', success='Temperature control log archived and reset'))
        else:
            return redirect(url_for('log_management', error='Temperature control log not found'))
    except Exception as e:
        print(f"[LOG] Error archiving temp log: {e}")
        return redirect(url_for('log_management', error=f'Error archiving log: {str(e)}'))


@app.route('/archive_log', methods=['POST'])
def archive_log():
    """Archive an application log file."""
    try:
        log_file = request.form.get('log_file')
        if not log_file:
            return redirect(url_for('log_management', error='No log file specified'))
        
        # Security: ensure log_file is just a filename, not a path
        if '/' in log_file or '\\' in log_file:
            return redirect(url_for('log_management', error='Invalid log file name'))
        
        log_path = os.path.join('logs', log_file)
        if not os.path.exists(log_path):
            return redirect(url_for('log_management', error=f'Log file not found: {log_file}'))
        
        # Create archive filename with timestamp
        archive_name = f"{log_path}.{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.archive"
        shutil.move(log_path, archive_name)
        
        # Create empty file to replace it
        open(log_path, 'w').close()
        
        print(f"[LOG] Archived {log_file} to {archive_name}")
        return redirect(url_for('log_management', success=f'Log file {log_file} archived'))
    except Exception as e:
        print(f"[LOG] Error archiving log: {e}")
        return redirect(url_for('log_management', error=f'Error archiving log: {str(e)}'))


@app.route('/delete_log', methods=['POST'])
def delete_log():
    """Delete an application log file."""
    try:
        log_file = request.form.get('log_file')
        if not log_file:
            return redirect(url_for('log_management', error='No log file specified'))
        
        # Security: ensure log_file is just a filename, not a path
        if '/' in log_file or '\\' in log_file:
            return redirect(url_for('log_management', error='Invalid log file name'))
        
        log_path = os.path.join('logs', log_file)
        if not os.path.exists(log_path):
            return redirect(url_for('log_management', error=f'Log file not found: {log_file}'))
        
        os.remove(log_path)
        print(f"[LOG] Deleted log file: {log_file}")
        return redirect(url_for('log_management', success=f'Log file {log_file} deleted'))
    except Exception as e:
        print(f"[LOG] Error deleting log: {e}")
        return redirect(url_for('log_management', error=f'Error deleting log: {str(e)}'))


@app.route('/export_batch_csv', methods=['POST'])
def export_batch_csv():
    """Export a batch's data to CSV."""
    try:
        import csv
        
        brewid = request.form.get('brewid')
        if not brewid:
            return redirect(url_for('log_management', error='No batch ID specified'))
        
        # Security: validate brewid format
        import re
        if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
            return redirect(url_for('log_management', error='Invalid batch ID format'))
        
        # Find batch file
        batch_file = os.path.join(BATCHES_DIR, f'{brewid}.jsonl')
        if not os.path.exists(batch_file):
            # Try legacy format
            legacy_files = glob_func(f'{BATCHES_DIR}/*{brewid}*.jsonl')
            if legacy_files:
                batch_file = legacy_files[0]
            else:
                return redirect(url_for('log_management', error=f'Batch file not found for ID: {brewid}'))
        
        # Create export directory
        export_dir = 'export'
        os.makedirs(export_dir, exist_ok=True)
        
        # Generate filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_filename = f'batch_{brewid}_{timestamp}.csv'
        csv_path = os.path.join(export_dir, csv_filename)
        
        # Read batch data
        data_rows = []
        with open(batch_file, 'r') as f:
            for line in f:
                if not line.strip():
                    continue
                try:
                    obj = json.loads(line)
                    # Handle both event/payload format and direct object format
                    if obj.get('event') == 'sample':
                        payload = obj.get('payload', {})
                        data_rows.append(payload)
                    elif obj.get('event') == 'batch_metadata':
                        # Skip metadata
                        continue
                    elif 'timestamp' in obj or 'gravity' in obj or 'temp_f' in obj:
                        # Legacy direct format
                        data_rows.append(obj)
                except Exception as e:
                    print(f"[LOG] Error parsing line in batch export: {e}")
                    continue
        
        # Write CSV
        if data_rows:
            with open(csv_path, 'w', newline='') as csvfile:
                # Define fieldnames
                fieldnames = ['timestamp', 'temp_f', 'gravity', 'color', 'brewid', 'beer_name', 'batch_name']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()
                for row in data_rows:
                    writer.writerow(row)
            
            return redirect(url_for('log_management', success=f'Batch exported to {csv_filename} ({len(data_rows)} records)'))
        else:
            return redirect(url_for('log_management', error='No data found in batch file'))
    except Exception as e:
        print(f"[LOG] Error exporting batch CSV: {e}")
        return redirect(url_for('log_management', error=f'Error exporting batch: {str(e)}'))


@app.route('/archive_batch', methods=['POST'])
def archive_batch():
    """Archive a batch file."""
    try:
        brewid = request.form.get('brewid')
        if not brewid:
            return redirect(url_for('log_management', error='No batch ID specified'))
        
        # Security: validate brewid format
        import re
        if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
            return redirect(url_for('log_management', error='Invalid batch ID format'))
        
        # Find batch file
        batch_file = os.path.join(BATCHES_DIR, f'{brewid}.jsonl')
        if not os.path.exists(batch_file):
            # Try legacy format
            legacy_files = glob_func(f'{BATCHES_DIR}/*{brewid}*.jsonl')
            if legacy_files:
                batch_file = legacy_files[0]
            else:
                return redirect(url_for('log_management', error=f'Batch file not found for ID: {brewid}'))
        
        # Create archive directory
        archive_dir = os.path.join(BATCHES_DIR, 'archive')
        os.makedirs(archive_dir, exist_ok=True)
        
        # Move file to archive with timestamp
        timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
        filename = os.path.basename(batch_file)
        archive_filename = f"{filename}.{timestamp}.archive"
        archive_path = os.path.join(archive_dir, archive_filename)
        
        shutil.move(batch_file, archive_path)
        print(f"[LOG] Archived batch {brewid} to {archive_path}")
        
        return redirect(url_for('log_management', success=f'Batch {brewid} archived'))
    except Exception as e:
        print(f"[LOG] Error archiving batch: {e}")
        return redirect(url_for('log_management', error=f'Error archiving batch: {str(e)}'))


@app.route('/close_batch', methods=['POST'])
def close_batch():
    """
    Close a batch by marking it as inactive in batch_history.
    This moves the batch from Current Activity to Batch History section.
    """
    try:
        brewid = request.form.get('brewid')
        color = request.form.get('color')
        
        if not brewid or not color:
            return jsonify({'success': False, 'error': 'Missing brewid or color'}), 400
        
        # Security: validate brewid format
        import re
        if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
            return jsonify({'success': False, 'error': 'Invalid batch ID format'}), 400
        
        # Load batch history for this color
        batch_history_file = f'batches/batch_history_{color}.json'
        batches = []
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
            except Exception as e:
                return jsonify({'success': False, 'error': f'Error reading batch history: {e}'}), 500
        
        # Find and update the batch
        batch_found = False
        for batch in batches:
            if batch.get('brewid') == brewid:
                batch['is_active'] = False
                batch['closed_date'] = datetime.utcnow().strftime('%Y-%m-%d')
                batch_found = True
                break
        
        if not batch_found:
            return jsonify({'success': False, 'error': 'Batch not found in history'}), 404
        
        # Save updated batch history
        try:
            with open(batch_history_file, 'w') as f:
                json.dump(batches, f, indent=2)
        except Exception as e:
            return jsonify({'success': False, 'error': f'Error saving batch history: {e}'}), 500
        
        # Clear this batch from tilt_cfg if it's currently assigned
        if color in tilt_cfg and tilt_cfg[color].get('brewid') == brewid:
            tilt_cfg[color] = {
                "beer_name": "",
                "batch_name": "",
                "ferm_start_date": "",
                "recipe_og": "",
                "recipe_fg": "",
                "recipe_abv": "",
                "actual_og": None,
                "brewid": "",
                "og_confirmed": False,
                "is_active": True,
                "closed_date": None,
                "notification_state": {
                    "fermentation_start_datetime": None,
                    "fermentation_completion_datetime": None,
                    "last_daily_report": None
                }
            }
            try:
                save_json(TILT_CONFIG_FILE, tilt_cfg)
            except Exception as e:
                print(f"[LOG] Error clearing tilt config: {e}")
        
        return jsonify({'success': True, 'message': f'Batch {brewid} closed successfully'})
    except Exception as e:
        print(f"[LOG] Error closing batch: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/reopen_batch', methods=['POST'])
def reopen_batch():
    """
    Reopen a closed batch by marking it as active again.
    This moves the batch from Batch History to Current Activity section.
    """
    try:
        brewid = request.form.get('brewid')
        color = request.form.get('color')
        
        if not brewid or not color:
            return jsonify({'success': False, 'error': 'Missing brewid or color'}), 400
        
        # Security: validate brewid format
        import re
        if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
            return jsonify({'success': False, 'error': 'Invalid batch ID format'}), 400
        
        # Load batch history for this color
        batch_history_file = f'batches/batch_history_{color}.json'
        batches = []
        if os.path.exists(batch_history_file):
            try:
                with open(batch_history_file, 'r') as f:
                    batches = json.load(f)
            except Exception as e:
                return jsonify({'success': False, 'error': f'Error reading batch history: {e}'}), 500
        
        # Find and update the batch
        batch_found = False
        for batch in batches:
            if batch.get('brewid') == brewid:
                batch['is_active'] = True
                batch['closed_date'] = None
                batch_found = True
                break
        
        if not batch_found:
            return jsonify({'success': False, 'error': 'Batch not found in history'}), 404
        
        # Save updated batch history
        try:
            with open(batch_history_file, 'w') as f:
                json.dump(batches, f, indent=2)
        except Exception as e:
            return jsonify({'success': False, 'error': f'Error saving batch history: {e}'}), 500
        
        return jsonify({'success': True, 'message': f'Batch {brewid} reopened successfully'})
    except Exception as e:
        print(f"[LOG] Error reopening batch: {e}")
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/delete_batch', methods=['POST'])
def delete_batch():
    """Delete a batch file."""
    try:
        brewid = request.form.get('brewid')
        if not brewid:
            return redirect(url_for('log_management', error='No batch ID specified'))
        
        # Security: validate brewid format
        import re
        if not re.match(r'^[a-zA-Z0-9\-_]+$', brewid):
            return redirect(url_for('log_management', error='Invalid batch ID format'))
        
        # Find batch file
        batch_file = os.path.join(BATCHES_DIR, f'{brewid}.jsonl')
        if not os.path.exists(batch_file):
            # Try legacy format
            legacy_files = glob_func(f'{BATCHES_DIR}/*{brewid}*.jsonl')
            if legacy_files:
                batch_file = legacy_files[0]
            else:
                return redirect(url_for('log_management', error=f'Batch file not found for ID: {brewid}'))
        
        os.remove(batch_file)
        print(f"[LOG] Deleted batch file: {brewid}")
        
        return redirect(url_for('log_management', success=f'Batch {brewid} deleted'))
    except Exception as e:
        print(f"[LOG] Error deleting batch: {e}")
        return redirect(url_for('log_management', error=f'Error deleting batch: {str(e)}'))


@app.route('/backup_system', methods=['POST'])
def backup_system():
    """Create a backup of all system files to the specified USB device."""
    import tarfile
    import shutil
    
    backup_path = request.form.get('backup_path', '/media/usb')
    
    # Validate that the backup path exists
    if not os.path.exists(backup_path):
        return jsonify({
            'success': False,
            'message': f'Backup path does not exist: {backup_path}. Please ensure USB device is mounted.'
        }), 400
    
    # Check if the path is writable
    if not os.access(backup_path, os.W_OK):
        return jsonify({
            'success': False,
            'message': f'Backup path is not writable: {backup_path}. Check permissions.'
        }), 400
    
    try:
        # Create timestamped backup filename
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_filename = f'fermenter_backup_{timestamp}.tar.gz'
        backup_full_path = os.path.join(backup_path, backup_filename)
        
        # Files and directories to backup
        items_to_backup = [
            # Python program files
            'app.py',
            'kasa_worker.py',
            'logger.py',
            'batch_history.py',
            'batch_storage.py',
            'fermentation_monitor.py',
            'tilt_static.py',
            'archive_compact_logs.py',
            'backfill_temp_control_jsonl.py',
            # Configuration files
            'config/',
            # Data files
            'batches/',
            'temp_control/',
            'temp_control_log.jsonl',
            # Web interface
            'templates/',
            'static/',
            # Documentation
            'requirements.txt',
            'start.sh',
            'README.md',
        ]
        
        # Create the tar.gz archive
        with tarfile.open(backup_full_path, 'w:gz') as tar:
            for item in items_to_backup:
                if os.path.exists(item):
                    tar.add(item)
        
        # Get the size of the backup file
        backup_size = os.path.getsize(backup_full_path)
        backup_size_mb = backup_size / (1024 * 1024)
        
        return jsonify({
            'success': True,
            'message': f'Backup created successfully: {backup_filename}',
            'filename': backup_filename,
            'size_mb': f'{backup_size_mb:.2f}',
            'path': backup_full_path
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Backup failed: {str(e)}'
        }), 500


@app.route('/restore_system', methods=['POST'])
def restore_system():
    """Restore system from a backup file."""
    import tarfile
    import shutil
    
    backup_path = request.form.get('backup_path', '/media/usb')
    backup_filename = request.form.get('backup_filename', '')
    
    if not backup_filename:
        return jsonify({
            'success': False,
            'message': 'No backup file specified.'
        }), 400
    
    # Security: Validate filename to prevent directory traversal
    if '..' in backup_filename or '/' in backup_filename or '\\' in backup_filename:
        return jsonify({
            'success': False,
            'message': 'Invalid backup filename.'
        }), 400
    
    # Security: Ensure filename has expected format
    if not backup_filename.startswith('fermenter_backup_') or not backup_filename.endswith('.tar.gz'):
        return jsonify({
            'success': False,
            'message': 'Invalid backup file format.'
        }), 400
    
    backup_full_path = os.path.join(backup_path, backup_filename)
    
    # Validate that the backup file exists
    if not os.path.exists(backup_full_path):
        return jsonify({
            'success': False,
            'message': f'Backup file does not exist: {backup_full_path}'
        }), 400
    
    try:
        # Create a secure temporary directory for extraction validation
        import tempfile
        temp_dir = tempfile.mkdtemp(prefix='fermenter_restore_')
        
        try:
            # Extract the backup to temporary directory first
            with tarfile.open(backup_full_path, 'r:gz') as tar:
                # Security check: ensure no absolute paths or parent directory references
                safe_members = []
                for member in tar.getmembers():
                    if member.name.startswith('/') or '..' in member.name:
                        shutil.rmtree(temp_dir)
                        return jsonify({
                            'success': False,
                            'message': f'Invalid backup file: contains unsafe paths'
                        }), 400
                    safe_members.append(member)
                
                # Extract only validated members to temp directory
                tar.extractall(temp_dir, members=safe_members)
            
            # Now copy files from temp to current directory
            # This allows us to validate before overwriting
            current_dir = os.getcwd()
            
            for item in os.listdir(temp_dir):
                src = os.path.join(temp_dir, item)
                dst = os.path.join(current_dir, item)
                
                # Backup existing files before overwriting
                if os.path.exists(dst):
                    backup_old = f'{dst}.backup_before_restore'
                    if os.path.isdir(dst):
                        if os.path.exists(backup_old):
                            shutil.rmtree(backup_old)
                        shutil.copytree(dst, backup_old)
                    else:
                        shutil.copy2(dst, backup_old)
                
                # Copy from temp to current
                if os.path.isdir(src):
                    if os.path.exists(dst):
                        shutil.rmtree(dst)
                    shutil.copytree(src, dst)
                else:
                    shutil.copy2(src, dst)
            
            return jsonify({
                'success': True,
                'message': f'System restored successfully from {backup_filename}. Please restart the application for changes to take effect.',
                'restart_required': True
            })
            
        finally:
            # Always cleanup temp directory
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir)
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Restore failed: {str(e)}'
        }), 500


@app.route('/list_backups', methods=['POST'])
def list_backups():
    """List available backup files in the specified directory."""
    backup_path = request.form.get('backup_path', '/media/usb')
    
    if not os.path.exists(backup_path):
        return jsonify({
            'success': False,
            'message': f'Backup path does not exist: {backup_path}',
            'backups': []
        })
    
    try:
        # List all .tar.gz files in the backup directory
        backups = []
        if os.path.isdir(backup_path):
            for filename in os.listdir(backup_path):
                if filename.startswith('fermenter_backup_') and filename.endswith('.tar.gz'):
                    full_path = os.path.join(backup_path, filename)
                    file_stat = os.stat(full_path)
                    backups.append({
                        'filename': filename,
                        'size_mb': f'{file_stat.st_size / (1024 * 1024):.2f}',
                        'modified': datetime.fromtimestamp(file_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
                    })
        
        # Sort by filename (which includes timestamp) in reverse order
        backups.sort(key=lambda x: x['filename'], reverse=True)
        
        return jsonify({
            'success': True,
            'backups': backups,
            'path': backup_path
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'Failed to list backups: {str(e)}',
            'backups': []
        })


@app.route('/exit_system', methods=['GET', 'POST'])
def exit_system():
    """
    Handle system exit:
    - GET: Show confirmation page
    - POST with confirm=yes: Turn off all plugs, show goodbye page, and shut down
    - POST with confirm=no: Return to main page
    """
    # Timing constants for shutdown sequence
    PLUG_COMMAND_DELAY = 0.5  # seconds to wait for plug command to be processed
    GOODBYE_DISPLAY_TIME = 2  # seconds to display goodbye page before shutdown
    PROCESS_TERMINATION_TIMEOUT = 2  # seconds to wait for process to terminate
    
    if request.method == 'POST':
        confirm = request.form.get('confirm', 'no')
        if confirm == 'yes':
            # Turn off all plugs before shutdown
            try:
                heating_plug = temp_cfg.get("heating_plug", "")
                cooling_plug = temp_cfg.get("cooling_plug", "")
                
                # Turn off heating plug if configured
                if heating_plug and kasa_queue:
                    kasa_queue.put({'mode': 'heating', 'url': heating_plug, 'action': 'off'})
                    time.sleep(PLUG_COMMAND_DELAY)
                
                # Turn off cooling plug if configured
                if cooling_plug and kasa_queue:
                    kasa_queue.put({'mode': 'cooling', 'url': cooling_plug, 'action': 'off'})
                    time.sleep(PLUG_COMMAND_DELAY)
            except Exception as e:
                print(f"[LOG] Error turning off plugs during shutdown: {e}")
            
            # Show goodbye page
            response = make_response(render_template('goodbye.html', brewery_name=system_cfg.get('brewery_name', 'Fermenter Temperature Controller')))
            
            # Schedule shutdown after response is sent
            def shutdown_system():
                time.sleep(GOODBYE_DISPLAY_TIME)
                try:
                    # Terminate the kasa worker process if it exists
                    if kasa_proc is not None:
                        try:
                            kasa_proc.terminate()
                            kasa_proc.join(timeout=PROCESS_TERMINATION_TIMEOUT)
                        except Exception:
                            pass
                except Exception as e:
                    print(f"[LOG] Error during process cleanup: {e}")
                
                # Shutdown Flask
                os.kill(os.getpid(), signal.SIGINT)
            
            # Start shutdown in background thread
            shutdown_thread = threading.Thread(target=shutdown_system)
            shutdown_thread.daemon = True
            shutdown_thread.start()
            
            return response
        else:
            # User cancelled, return to main page
            return redirect('/')
    
    # GET request: show confirmation page
    return render_template('exit_system.html')


# --- Program entry ---------------------------------------------------------
def open_browser():
    """
    Open the default web browser to the Flask app URL after a short delay.
    This runs in a separate thread to avoid blocking the Flask startup.
    """
    time.sleep(1.5)  # Wait for Flask to start
    try:
        webbrowser.open('http://127.0.0.1:5000')
        print("[LOG] Opened browser at http://127.0.0.1:5000")
    except Exception as e:
        print(f"[LOG] Could not automatically open browser: {e}")
        print("[LOG] Please manually navigate to http://127.0.0.1:5000")


if __name__ == '__main__':
    try:
        os.makedirs(BATCHES_DIR, exist_ok=True)
    except Exception:
        pass

    # Start a thread to open the browser after Flask starts
    # Only open browser in the main process (not in Werkzeug reloader child process)
    if os.environ.get('WERKZEUG_RUN_MAIN') != 'true':
        browser_thread = threading.Thread(target=open_browser, daemon=True)
        browser_thread.start()

    # Run the Flask app
    app.run(host='0.0.0.0', port=5000, debug=True)

